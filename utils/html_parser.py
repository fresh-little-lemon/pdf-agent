import os
import base64
import time
from typing import List
from openai import OpenAI
from bs4 import BeautifulSoup
import re


def encode_image(image_path: str) -> str:
    """å°†å›¾ç‰‡è½¬æ¢ä¸ºbase64ç¼–ç """
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode("utf-8")


def inference_with_api(image_path: str, prompt: str, sys_prompt: str = "You are a helpful assistant.", 
                      model_id: str = "Qwen/Qwen2.5-VL-72B-Instruct", 
                      min_pixels: int = 512*28*28, max_pixels: int = 2048*28*28,
                      max_retries: int = 3, retry_delay: float = 1.0) -> str:
    """
    ä½¿ç”¨APIè°ƒç”¨Qwen2.5-VLæ¨¡å‹è¿›è¡Œå›¾ç‰‡è§£æ
    
    Args:
        image_path: å›¾ç‰‡è·¯å¾„
        prompt: æç¤ºè¯
        sys_prompt: ç³»ç»Ÿæç¤ºè¯
        model_id: æ¨¡å‹ID
        min_pixels: æœ€å°åƒç´ æ•°
        max_pixels: æœ€å¤§åƒç´ æ•°
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        retry_delay: é‡è¯•é—´éš”ï¼ˆç§’ï¼‰
    
    Returns:
        æ¨¡å‹è¾“å‡ºçš„HTMLå†…å®¹
    """
    base64_image = encode_image(image_path)
    
    # ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥
    api_key = os.getenv("MODELSCOPE_SDK_TOKEN") or os.getenv("DASHSCOPE_API_KEY")
    if not api_key:
        raise Exception("è¯·è®¾ç½® MODELSCOPE_SDK_TOKEN æˆ– DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡")
    
    client = OpenAI(
        api_key=api_key,
        base_url="https://api-inference.modelscope.cn/v1/"
    )

    messages = [
        {
            "role": "system",
            "content": [{"type": "text", "text": sys_prompt}]
        },
        {
            "role": "user",
            "content": [
                {
                    "type": "image_url",
                    "min_pixels": min_pixels,
                    "max_pixels": max_pixels,
                    "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"},
                },
                {"type": "text", "text": prompt},
            ],
        }
    ]
    
    # æ·»åŠ é‡è¯•æœºåˆ¶
    last_exception = None
    for attempt in range(max_retries + 1):
        try:
            completion = client.chat.completions.create(
                model=model_id,
                messages=messages,
            )
            
            result = completion.choices[0].message.content
            if result and result.strip():  # æ£€æŸ¥ç»“æœæ˜¯å¦æœ‰æ•ˆ
                if attempt > 0:
                    print(f"âœ… APIè°ƒç”¨æˆåŠŸï¼ˆç¬¬{attempt + 1}æ¬¡å°è¯•ï¼‰")
                return result
            else:
                raise Exception("APIè¿”å›ç©ºç»“æœ")
                
        except Exception as e:
            last_exception = e
            if attempt < max_retries:
                print(f"âš ï¸ APIè°ƒç”¨å¤±è´¥ï¼ˆç¬¬{attempt + 1}æ¬¡å°è¯•ï¼‰: {str(e)}")
                print(f"ğŸ”„ ç­‰å¾… {retry_delay} ç§’åé‡è¯•...")
                time.sleep(retry_delay)
                # æ¯æ¬¡é‡è¯•å¢åŠ å»¶è¿Ÿæ—¶é—´ï¼Œé¿å…é¢‘ç¹è¯·æ±‚
                retry_delay *= 1.5
            else:
                print(f"âŒ APIè°ƒç”¨å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries + 1})")
    
    # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºæœ€åä¸€ä¸ªå¼‚å¸¸
    raise last_exception


def clean_and_format_html(full_predict: str) -> str:
    """
    æ¸…ç†å’Œæ ¼å¼åŒ–HTMLå†…å®¹
    
    Args:
        full_predict: åŸå§‹HTMLé¢„æµ‹ç»“æœ
    
    Returns:
        æ¸…ç†åçš„HTMLå†…å®¹
    """
    soup = BeautifulSoup(full_predict, 'html.parser')
    
    # æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼åŒ¹é…é¢œè‰²æ ·å¼
    color_pattern = re.compile(r'\bcolor:[^;]+;?')

    # æŸ¥æ‰¾æ‰€æœ‰å¸¦æœ‰styleå±æ€§çš„æ ‡ç­¾å¹¶ç§»é™¤é¢œè‰²æ ·å¼
    for tag in soup.find_all(style=True):
        original_style = tag.get('style', '')
        new_style = color_pattern.sub('', original_style)
        if not new_style.strip():
            del tag['style']
        else:
            new_style = new_style.rstrip(';')
            tag['style'] = new_style
            
    # ç§»é™¤data-bboxå’Œdata-polygonå±æ€§
    for attr in ["data-bbox", "data-polygon"]:
        for tag in soup.find_all(attrs={attr: True}):
            del tag[attr]

    classes_to_update = ['formula.machine_printed', 'formula.handwritten']
    # æ›´æ–°ç‰¹å®šçš„ç±»å
    for tag in soup.find_all(class_=True):
        if hasattr(tag, 'attrs') and 'class' in tag.attrs:
            new_classes = [cls if cls not in classes_to_update else 'formula' for cls in tag.get('class', [])]
            tag['class'] = list(dict.fromkeys(new_classes))  # å»é‡å¹¶æ›´æ–°ç±»å

    # æ¸…ç†ç‰¹å®šç±»åçš„divå†…å®¹
    for div in soup.find_all('div', class_='image caption'):
        div.clear()
        div['class'] = ['image']

    classes_to_clean = ['music sheet', 'chemical formula', 'chart']
    # æ¸…ç†ç‰¹å®šç±»åçš„æ ‡ç­¾å†…å®¹å¹¶ç§»é™¤formatå±æ€§
    for class_name in classes_to_clean:
        for tag in soup.find_all(class_=class_name):
            if hasattr(tag, 'clear'):
                tag.clear()
                if 'format' in tag.attrs:
                    del tag['format']

    # æ‰‹åŠ¨æ„å»ºè¾“å‡ºå­—ç¬¦ä¸²
    output = []
    if soup.body:
        for child in soup.body.children:
            if hasattr(child, 'name'):  # Tag object
                output.append(str(child))
                output.append('\n')
            elif isinstance(child, str) and not child.strip():
                continue  # å¿½ç•¥ç©ºç™½æ–‡æœ¬èŠ‚ç‚¹
    
    complete_html = f"""```html\n<html><body>\n{" ".join(output)}</body></html>\n```"""
    return complete_html


def parse_images_to_html(image_paths: List[str], pdf_filename: str, output_dir: str = "tmp", start_page: int = 1, enable_clean: bool = False, max_retries: int = 3, retry_delay: float = 1.0) -> List[str]:
    """
    å°†å›¾ç‰‡åˆ—è¡¨è§£æä¸ºHTMLæ ¼å¼å¹¶ä¿å­˜
    
    Args:
        image_paths: å›¾ç‰‡è·¯å¾„åˆ—è¡¨
        pdf_filename: PDFæ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
        output_dir: è¾“å‡ºç›®å½•
        start_page: èµ·å§‹é¡µç ï¼Œé»˜è®¤ä¸º1
        enable_clean: æ˜¯å¦å¯ç”¨HTMLæ¸…ç†åŠŸèƒ½ï¼Œé»˜è®¤ä¸ºFalse
        max_retries: æ¯ä¸ªé¡µé¢çš„æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤ä¸º3
        retry_delay: é‡è¯•é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ä¸º1.0
    
    Returns:
        ç”Ÿæˆçš„HTMLæ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    # åˆ›å»ºHTMLè¾“å‡ºç›®å½•
    html_output_dir = os.path.join(output_dir, f"{pdf_filename}_html")
    if not os.path.exists(html_output_dir):
        os.makedirs(html_output_dir)
    
    system_prompt = "You are an AI specialized in recognizing and extracting text from images. Your mission is to analyze the image document and generate the result in QwenVL Document Parser HTML format using specified tags while maintaining user privacy and data integrity."
    prompt = "QwenVL HTML"
    
    html_files = []
    
    for i, image_path in enumerate(image_paths):
        page_number = start_page + i
        try:
            print(f"æ­£åœ¨è§£æç¬¬ {page_number} é¡µå›¾ç‰‡...")
            
            # è°ƒç”¨APIè¿›è¡Œè§£æ
            raw_html = inference_with_api(
                image_path=image_path,
                prompt=prompt,
                sys_prompt=system_prompt,
                max_retries=max_retries,
                retry_delay=retry_delay
            )
            
            # æ ¹æ®è®¾ç½®å†³å®šæ˜¯å¦æ¸…ç†å’Œæ ¼å¼åŒ–HTML
            if enable_clean:
                final_html = clean_and_format_html(raw_html)
            else:
                final_html = raw_html
            
            # ç”ŸæˆHTMLæ–‡ä»¶å
            html_filename = f"page_{page_number}.html"
            html_path = os.path.join(html_output_dir, html_filename)
            
            # ä¿å­˜HTMLæ–‡ä»¶
            with open(html_path, 'w', encoding='utf-8') as f:
                f.write(final_html)
            
            html_files.append(html_path)
            print(f"ç¬¬ {page_number} é¡µè§£æå®Œæˆï¼Œä¿å­˜åˆ°: {html_path}")
            
        except Exception as e:
            print(f"è§£æç¬¬ {page_number} é¡µæ—¶å‡ºé”™: {str(e)}")
            continue
    
    return html_files


def get_api_status() -> dict:
    """
    æ£€æŸ¥APIçŠ¶æ€å’Œé…ç½®
    
    Returns:
        APIçŠ¶æ€ä¿¡æ¯
    """
    api_key = os.getenv("MODELSCOPE_SDK_TOKEN") or os.getenv("DASHSCOPE_API_KEY")
    
    return {
        "api_key_configured": bool(api_key),
        "api_key_length": len(api_key) if api_key else 0,
        "base_url": "https://api-inference.modelscope.cn/v1/"
    }


def sequential_parse_images_to_html(image_paths: List[str], pdf_filename: str, output_dir: str = "tmp", enable_clean: bool = False, max_retries: int = 3, retry_delay: float = 1.0) -> List[str]:
    """
    é¡ºåºè§£æå›¾ç‰‡ä¸ºHTMLï¼ˆæ¨èæ–¹å¼ï¼Œé¡µç å¯¹é½ä¸”ä¸ä¼šè¦†ç›–ï¼‰
    
    Args:
        image_paths: å›¾ç‰‡è·¯å¾„åˆ—è¡¨
        pdf_filename: PDFæ–‡ä»¶å
        output_dir: è¾“å‡ºç›®å½•
        enable_clean: æ˜¯å¦å¯ç”¨HTMLæ¸…ç†åŠŸèƒ½
        max_retries: æ¯ä¸ªé¡µé¢çš„æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤ä¸º3
        retry_delay: é‡è¯•é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ä¸º1.0
    
    Returns:
        ç”Ÿæˆçš„HTMLæ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    return parse_images_to_html(image_paths, pdf_filename, output_dir, start_page=1, enable_clean=enable_clean, max_retries=max_retries, retry_delay=retry_delay)


def parallel_parse_images_to_html(image_paths: List[str], pdf_filename: str, output_dir: str = "tmp", 
                                  max_workers: int = 3, enable_clean: bool = False, max_retries: int = 3, retry_delay: float = 1.0) -> List[str]:
    """
    å¹¶è¡Œè§£æå›¾ç‰‡ä¸ºHTMLï¼ˆæ³¨æ„ï¼šéœ€è¦ç¡®ä¿APIæ”¯æŒå¹¶å‘è°ƒç”¨ï¼‰
    
    Args:
        image_paths: å›¾ç‰‡è·¯å¾„åˆ—è¡¨
        pdf_filename: PDFæ–‡ä»¶å
        output_dir: è¾“å‡ºç›®å½•
        max_workers: æœ€å¤§å¹¶è¡Œå·¥ä½œæ•°
        enable_clean: æ˜¯å¦å¯ç”¨HTMLæ¸…ç†åŠŸèƒ½
        max_retries: æ¯ä¸ªé¡µé¢çš„æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤ä¸º3
        retry_delay: é‡è¯•é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ä¸º1.0
    
    Returns:
        ç”Ÿæˆçš„HTMLæ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    import concurrent.futures
    
    # åˆ›å»ºHTMLè¾“å‡ºç›®å½•
    html_output_dir = os.path.join(output_dir, f"{pdf_filename}_html")
    if not os.path.exists(html_output_dir):
        os.makedirs(html_output_dir)
    
    system_prompt = "You are an AI specialized in recognizing and extracting text from images. Your mission is to analyze the image document and generate the result in QwenVL Document Parser HTML format using specified tags while maintaining user privacy and data integrity. "
    prompt = "QwenVL HTML"
    
    def process_single_image(args):
        image_path, page_number = args
        try:
            print(f"æ­£åœ¨è§£æç¬¬ {page_number} é¡µå›¾ç‰‡...")
            
            # è°ƒç”¨APIè¿›è¡Œè§£æ
            raw_html = inference_with_api(
                image_path=image_path,
                prompt=prompt,
                sys_prompt=system_prompt,
                max_retries=max_retries,
                retry_delay=retry_delay
            )
            
            # æ ¹æ®è®¾ç½®å†³å®šæ˜¯å¦æ¸…ç†å’Œæ ¼å¼åŒ–HTML
            if enable_clean:
                final_html = clean_and_format_html(raw_html)
            else:
                final_html = raw_html
            
            # ç”ŸæˆHTMLæ–‡ä»¶å
            html_filename = f"page_{page_number}.html"
            html_path = os.path.join(html_output_dir, html_filename)
            
            # ä¿å­˜HTMLæ–‡ä»¶
            with open(html_path, 'w', encoding='utf-8') as f:
                f.write(final_html)
            
            print(f"ç¬¬ {page_number} é¡µè§£æå®Œæˆï¼Œä¿å­˜åˆ°: {html_path}")
            return html_path
            
        except Exception as e:
            print(f"è§£æç¬¬ {page_number} é¡µæ—¶å‡ºé”™: {str(e)}")
            return None
    
    # å‡†å¤‡å‚æ•°ï¼š(image_path, page_number)
    args_list = [(image_path, i + 1) for i, image_path in enumerate(image_paths)]
    
    html_files = []
    
    # ä½¿ç”¨çº¿ç¨‹æ± è¿›è¡Œå¹¶è¡Œå¤„ç†
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_page = {executor.submit(process_single_image, args): args[1] for args in args_list}
        
        # è·å–ç»“æœ
        for future in concurrent.futures.as_completed(future_to_page):
            page_number = future_to_page[future]
            try:
                html_path = future.result()
                if html_path:
                    html_files.append(html_path)
            except Exception as e:
                print(f"ç¬¬ {page_number} é¡µå¤„ç†å¼‚å¸¸: {str(e)}")
    
    # æŒ‰é¡µç æ’åº
    html_files.sort(key=lambda x: int(os.path.basename(x).split('_')[1].split('.')[0]))
    
    return html_files


def parse_all_images_to_html(image_paths: List[str], pdf_filename: str, output_dir: str = "tmp", 
                            parallel: bool = False, max_workers: int = 3, enable_clean: bool = False, max_retries: int = 3, retry_delay: float = 1.0) -> List[str]:
    """
    è§£ææ‰€æœ‰å›¾ç‰‡ä¸ºHTMLæ ¼å¼ï¼ˆæ”¯æŒä¸²è¡Œå’Œå¹¶è¡Œå¤„ç†ï¼‰
    
    Args:
        image_paths: å›¾ç‰‡è·¯å¾„åˆ—è¡¨
        pdf_filename: PDFæ–‡ä»¶å
        output_dir: è¾“å‡ºç›®å½•
        parallel: æ˜¯å¦ä½¿ç”¨å¹¶è¡Œå¤„ç†
        max_workers: å¹¶è¡Œå¤„ç†çš„æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
        enable_clean: æ˜¯å¦å¯ç”¨HTMLæ¸…ç†åŠŸèƒ½
        max_retries: æ¯ä¸ªé¡µé¢çš„æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤ä¸º3
        retry_delay: é‡è¯•é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ä¸º1.0
    
    Returns:
        ç”Ÿæˆçš„HTMLæ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    if parallel:
        print(f"ä½¿ç”¨å¹¶è¡Œå¤„ç†æ¨¡å¼ï¼Œ{max_workers}ä¸ªçº¿ç¨‹...")
        return parallel_parse_images_to_html(image_paths, pdf_filename, output_dir, max_workers, enable_clean, max_retries, retry_delay)
    else:
        print("ä½¿ç”¨ä¸²è¡Œå¤„ç†æ¨¡å¼...")
        return sequential_parse_images_to_html(image_paths, pdf_filename, output_dir, enable_clean, max_retries, retry_delay)


def insert_extracted_images_to_html(html_files: List[str], extracted_images_dir: str, pdf_filename: str) -> List[str]:
    """
    å°†æå–çš„å›¾ç‰‡æ’å…¥åˆ°å¯¹åº”é¡µçš„HTMLä¸­çš„imgå…ƒç´ ä½ç½®
    
    Args:
        html_files: HTMLæ–‡ä»¶è·¯å¾„åˆ—è¡¨
        extracted_images_dir: æå–å›¾ç‰‡çš„ç›®å½•è·¯å¾„
        pdf_filename: PDFæ–‡ä»¶å
    
    Returns:
        æ›´æ–°åçš„HTMLæ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    import re
    
    updated_html_files = []
    
    # è·å–æå–å›¾ç‰‡çš„æ–‡ä»¶å¤¹è·¯å¾„
    figure_dir = os.path.join(extracted_images_dir, f"{pdf_filename}_figure")
    
    if not os.path.exists(figure_dir):
        print(f"è­¦å‘Šï¼šå›¾ç‰‡æ–‡ä»¶å¤¹ {figure_dir} ä¸å­˜åœ¨")
        return html_files
    
    # è·å–æ‰€æœ‰æå–çš„å›¾ç‰‡
    extracted_images = {}
    if os.path.exists(figure_dir):
        for img_file in os.listdir(figure_dir):
            if img_file.startswith(pdf_filename):
                # è§£ææ–‡ä»¶åï¼š{pdfæ–‡ä»¶å}_page_{é¡µç }_{å›¾ç‰‡åºå·}.{æ‰©å±•å}
                pattern = rf"{re.escape(pdf_filename)}_page_(\d+)_(\d+)\.(jpg|jpeg|png|gif|bmp)"
                match = re.match(pattern, img_file)
                if match:
                    page_num = int(match.group(1))
                    img_index = int(match.group(2))
                    img_path = os.path.abspath(os.path.join(figure_dir, img_file))
                    
                    if page_num not in extracted_images:
                        extracted_images[page_num] = {}
                    extracted_images[page_num][img_index] = img_path
    
    # å¤„ç†æ¯ä¸ªHTMLæ–‡ä»¶
    for html_file in html_files:
        try:
            # ä»æ–‡ä»¶åä¸­æå–é¡µç 
            html_filename = os.path.basename(html_file)
            page_match = re.match(r'page_(\d+)\.html', html_filename)
            if not page_match:
                updated_html_files.append(html_file)
                continue
            
            page_num = int(page_match.group(1))
            
            # è¯»å–HTMLæ–‡ä»¶
            with open(html_file, 'r', encoding='utf-8') as f:
                html_content = f.read()
            
            # è§£æHTML
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # æŸ¥æ‰¾æ‰€æœ‰imgå…ƒç´ 
            img_tags = soup.find_all('img')
            
            if page_num in extracted_images and img_tags:
                # ä¸ºæ¯ä¸ªimgæ ‡ç­¾æ·»åŠ srcå±æ€§
                available_images = extracted_images[page_num]
                img_index = 1
                
                for img_tag in img_tags:
                    if img_index in available_images:
                        # æ·»åŠ srcå±æ€§ï¼Œä½¿ç”¨ç»å¯¹è·¯å¾„
                        img_path = available_images[img_index]
                        img_tag['src'] = img_path
                        print(f"ä¸ºç¬¬{page_num}é¡µç¬¬{img_index}å¼ å›¾ç‰‡æ·»åŠ è·¯å¾„: {img_path}")
                        img_index += 1
                    else:
                        print(f"è­¦å‘Šï¼šç¬¬{page_num}é¡µç¬¬{img_index}å¼ å›¾ç‰‡æœªæ‰¾åˆ°å¯¹åº”çš„æå–å›¾ç‰‡")
                        img_index += 1
            
            # ä¿å­˜æ›´æ–°åçš„HTMLæ–‡ä»¶
            updated_html = str(soup)
            with open(html_file, 'w', encoding='utf-8') as f:
                f.write(updated_html)
            
            updated_html_files.append(html_file)
            print(f"âœ… æ›´æ–°HTMLæ–‡ä»¶: {html_file}")
            
        except Exception as e:
            print(f"âŒ æ›´æ–°HTMLæ–‡ä»¶ {html_file} æ—¶å‡ºé”™: {str(e)}")
            updated_html_files.append(html_file)
    
    return updated_html_files


def parse_and_insert_images(pdf_file_bytes: bytes, pdf_filename: str, output_dir: str = "tmp", 
                           parallel: bool = False, max_workers: int = 3, enable_clean: bool = False,
                           insert_extracted_images: bool = False, max_retries: int = 3, retry_delay: float = 1.0) -> dict:
    """
    å®Œæ•´çš„PDFè§£ææµç¨‹ï¼šè½¬æ¢ä¸ºå›¾ç‰‡ã€è§£æä¸ºHTMLã€å¯é€‰æ’å…¥æå–çš„å›¾ç‰‡
    
    Args:
        pdf_file_bytes: PDFæ–‡ä»¶å­—èŠ‚æ•°æ®
        pdf_filename: PDFæ–‡ä»¶å
        output_dir: è¾“å‡ºç›®å½•
        parallel: æ˜¯å¦ä½¿ç”¨å¹¶è¡Œå¤„ç†
        max_workers: å·¥ä½œçº¿ç¨‹æ•°
        enable_clean: æ˜¯å¦å¯ç”¨HTMLæ¸…ç†
        insert_extracted_images: æ˜¯å¦æ’å…¥æå–çš„å›¾ç‰‡åˆ°HTMLä¸­
        max_retries: æ¯ä¸ªé¡µé¢çš„æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤ä¸º3
        retry_delay: é‡è¯•é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ä¸º1.0
    
    Returns:
        åŒ…å«æ‰€æœ‰ç»“æœè·¯å¾„çš„å­—å…¸
    """
    from utils.pdf_converter import pdf_to_jpg
    from utils.image_extractor import extract_images_from_pdf
    
    results = {
        'converted_images': [],
        'extracted_images': [],
        'html_files': [],
        'status': 'success',
        'message': ''
    }
    
    try:
        # æ­¥éª¤1: è½¬æ¢PDFä¸ºå›¾ç‰‡
        print("æ­¥éª¤1/4: è½¬æ¢PDFä¸ºå›¾ç‰‡...")
        converted_images = pdf_to_jpg(
            pdf_file_bytes,
            pdf_filename=pdf_filename,
            output_dir=output_dir,
            dpi=150
        )
        results['converted_images'] = converted_images
        print(f"âœ… PDFè½¬æ¢å®Œæˆï¼Œå…±ç”Ÿæˆ {len(converted_images)} å¼ å›¾ç‰‡")
        
        # æ­¥éª¤2: è§£æå›¾ç‰‡ä¸ºHTML
        print("æ­¥éª¤2/4: è§£æå›¾ç‰‡ä¸ºHTML...")
        html_files = parse_all_images_to_html(
            image_paths=converted_images,
            pdf_filename=pdf_filename,
            output_dir=output_dir,
            parallel=parallel,
            max_workers=max_workers,
            enable_clean=enable_clean,
            max_retries=max_retries,
            retry_delay=retry_delay
        )
        results['html_files'] = html_files
        print(f"âœ… HTMLè§£æå®Œæˆï¼Œå…±ç”Ÿæˆ {len(html_files)} ä¸ªHTMLæ–‡ä»¶")
        
        # æ­¥éª¤3: æå–PDFä¸­çš„å›¾ç‰‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if insert_extracted_images:
            print("æ­¥éª¤3/4: æå–PDFä¸­çš„å›¾ç‰‡...")
            extracted_images = extract_images_from_pdf(
                pdf_file_bytes,
                pdf_filename,
                output_dir
            )
            results['extracted_images'] = extracted_images
            print(f"âœ… å›¾ç‰‡æå–å®Œæˆï¼Œå…±æå– {len(extracted_images)} å¼ å›¾ç‰‡")
            
            # æ­¥éª¤4: å°†æå–çš„å›¾ç‰‡æ’å…¥åˆ°HTMLä¸­
            print("æ­¥éª¤4/4: å°†æå–çš„å›¾ç‰‡æ’å…¥åˆ°HTMLä¸­...")
            updated_html_files = insert_extracted_images_to_html(
                html_files,
                output_dir,
                pdf_filename
            )
            results['html_files'] = updated_html_files
            print(f"âœ… å›¾ç‰‡æ’å…¥å®Œæˆï¼Œæ›´æ–°äº† {len(updated_html_files)} ä¸ªHTMLæ–‡ä»¶")
        else:
            print("è·³è¿‡å›¾ç‰‡æå–å’Œæ’å…¥æ­¥éª¤")
        
        results['message'] = "æ‰€æœ‰æ­¥éª¤å®ŒæˆæˆåŠŸ"
        
    except Exception as e:
        results['status'] = 'error'
        results['message'] = f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}"
        print(f"âŒ é”™è¯¯: {str(e)}")
    
    return results


def batch_parse_images_to_html(image_paths: List[str], pdf_filename: str, output_dir: str = "tmp", 
                               batch_size: int = 5, use_parallel: bool = False, enable_clean: bool = False, max_retries: int = 3, retry_delay: float = 1.0) -> List[str]:
    """
    æ‰¹é‡è§£æå›¾ç‰‡ä¸ºHTMLï¼ˆå…¼å®¹æ€§å‡½æ•°ï¼Œç°åœ¨ä½¿ç”¨é¡ºåºå¤„ç†ç¡®ä¿é¡µç æ­£ç¡®ï¼‰
    
    Args:
        image_paths: å›¾ç‰‡è·¯å¾„åˆ—è¡¨
        pdf_filename: PDFæ–‡ä»¶å
        output_dir: è¾“å‡ºç›®å½•
        batch_size: å·²åºŸå¼ƒï¼Œä¿ç•™å…¼å®¹æ€§
        use_parallel: æ˜¯å¦ä½¿ç”¨å¹¶è¡Œå¤„ç†
        enable_clean: æ˜¯å¦å¯ç”¨HTMLæ¸…ç†åŠŸèƒ½
        max_retries: æ¯ä¸ªé¡µé¢çš„æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤ä¸º3
        retry_delay: é‡è¯•é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ä¸º1.0
    
    Returns:
        ç”Ÿæˆçš„HTMLæ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    if use_parallel:
        print("ä½¿ç”¨å¹¶è¡Œå¤„ç†æ¨¡å¼...")
        return parallel_parse_images_to_html(image_paths, pdf_filename, output_dir, max_workers=3, enable_clean=enable_clean, max_retries=max_retries, retry_delay=retry_delay)
    else:
        print("ä½¿ç”¨é¡ºåºå¤„ç†æ¨¡å¼...")
        return sequential_parse_images_to_html(image_paths, pdf_filename, output_dir, enable_clean=enable_clean, max_retries=max_retries, retry_delay=retry_delay) 