{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6ad3b52",
   "metadata": {},
   "source": [
    "### Spatial Understanding with Qwen2.5-VL\n",
    "\n",
    "This notebook showcases Qwen2.5-VL's advanced spatial localization abilities, including accurate object detection and specific target grounding within images. \n",
    "\n",
    "See how it integrates visual and linguistic understanding to interpret complex scenes effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14ee4e7-3706-45c2-9fd1-cc49b0f00fd0",
   "metadata": {},
   "source": [
    "Prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d7a1ed6-b782-4516-874f-8864fa13da56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/huggingface/transformers\n",
    "# !pip install qwen-vl-utils\n",
    "# !pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c878bf",
   "metadata": {},
   "source": [
    "#### \\[Setup\\]\n",
    "\n",
    "Load visualization utils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07044e07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-29T12:15:56.383829Z",
     "iopub.status.busy": "2025-01-29T12:15:56.383261Z",
     "iopub.status.idle": "2025-01-29T12:15:58.004390Z",
     "shell.execute_reply": "2025-01-29T12:15:58.003489Z",
     "shell.execute_reply.started": "2025-01-29T12:15:56.383805Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "Suggested packages:\n",
      "  fonts-noto-cjk-extra\n",
      "The following NEW packages will be installed:\n",
      "  fonts-noto-cjk\n",
      "0 upgraded, 1 newly installed, 0 to remove and 41 not upgraded.\n",
      "Need to get 55.7 MB of archives.\n",
      "After this operation, 92.0 MB of additional disk space will be used.\n",
      "Get:1 https://mirrors.aliyun.com/ubuntu focal/main amd64 fonts-noto-cjk all 1:20190410+repack1-2 [55.7 MB]\n",
      "Fetched 55.7 MB in 58s (956 kB/s)                                              \n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package fonts-noto-cjk.\n",
      "(Reading database ... 27798 files and directories currently installed.)\n",
      "Preparing to unpack .../fonts-noto-cjk_1%3a20190410+repack1-2_all.deb ...\n",
      "Unpacking fonts-noto-cjk (1:20190410+repack1-2) ...\n",
      "Setting up fonts-noto-cjk (1:20190410+repack1-2) ...\n",
      "Processing triggers for fontconfig (2.13.1-2ubuntu3) ...\n"
     ]
    }
   ],
   "source": [
    "# @title Plotting Util\n",
    "\n",
    "# Get Noto JP font to display janapese characters\n",
    "!apt-get install fonts-noto-cjk  # For Noto Sans CJK JP\n",
    "\n",
    "#!apt-get install fonts-source-han-sans-jp # For Source Han Sans (Japanese)\n",
    "\n",
    "import json\n",
    "import random\n",
    "import io\n",
    "import ast\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from PIL import ImageColor\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "additional_colors = [colorname for (colorname, colorcode) in ImageColor.colormap.items()]\n",
    "\n",
    "def decode_xml_points(text):\n",
    "    try:\n",
    "        root = ET.fromstring(text)\n",
    "        num_points = (len(root.attrib) - 1) // 2\n",
    "        points = []\n",
    "        for i in range(num_points):\n",
    "            x = root.attrib.get(f'x{i+1}')\n",
    "            y = root.attrib.get(f'y{i+1}')\n",
    "            points.append([x, y])\n",
    "        alt = root.attrib.get('alt')\n",
    "        phrase = root.text.strip() if root.text else None\n",
    "        return {\n",
    "            \"points\": points,\n",
    "            \"alt\": alt,\n",
    "            \"phrase\": phrase\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def plot_bounding_boxes(im, bounding_boxes, input_width, input_height):\n",
    "    \"\"\"\n",
    "    Plots bounding boxes on an image with markers for each a name, using PIL, normalized coordinates, and different colors.\n",
    "\n",
    "    Args:\n",
    "        img_path: The path to the image file.\n",
    "        bounding_boxes: A list of bounding boxes containing the name of the object\n",
    "         and their positions in normalized [y1 x1 y2 x2] format.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the image\n",
    "    img = im\n",
    "    width, height = img.size\n",
    "    print(img.size)\n",
    "    # Create a drawing object\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    # Define a list of colors\n",
    "    colors = [\n",
    "    'red',\n",
    "    'green',\n",
    "    'blue',\n",
    "    'yellow',\n",
    "    'orange',\n",
    "    'pink',\n",
    "    'purple',\n",
    "    'brown',\n",
    "    'gray',\n",
    "    'beige',\n",
    "    'turquoise',\n",
    "    'cyan',\n",
    "    'magenta',\n",
    "    'lime',\n",
    "    'navy',\n",
    "    'maroon',\n",
    "    'teal',\n",
    "    'olive',\n",
    "    'coral',\n",
    "    'lavender',\n",
    "    'violet',\n",
    "    'gold',\n",
    "    'silver',\n",
    "    ] + additional_colors\n",
    "\n",
    "    # Parsing out the markdown fencing\n",
    "    bounding_boxes = parse_json(bounding_boxes)\n",
    "\n",
    "    font = ImageFont.truetype(\"NotoSansCJK-Regular.ttc\", size=14)\n",
    "\n",
    "    try:\n",
    "      json_output = ast.literal_eval(bounding_boxes)\n",
    "    except Exception as e:\n",
    "      end_idx = bounding_boxes.rfind('\"}') + len('\"}')\n",
    "      truncated_text = bounding_boxes[:end_idx] + \"]\"\n",
    "      json_output = ast.literal_eval(truncated_text)\n",
    "\n",
    "    # Iterate over the bounding boxes\n",
    "    for i, bounding_box in enumerate(json_output):\n",
    "      # Select a color from the list\n",
    "      color = colors[i % len(colors)]\n",
    "\n",
    "      # Convert normalized coordinates to absolute coordinates\n",
    "      abs_y1 = int(bounding_box[\"bbox_2d\"][1]/input_height * height)\n",
    "      abs_x1 = int(bounding_box[\"bbox_2d\"][0]/input_width * width)\n",
    "      abs_y2 = int(bounding_box[\"bbox_2d\"][3]/input_height * height)\n",
    "      abs_x2 = int(bounding_box[\"bbox_2d\"][2]/input_width * width)\n",
    "\n",
    "      if abs_x1 > abs_x2:\n",
    "        abs_x1, abs_x2 = abs_x2, abs_x1\n",
    "\n",
    "      if abs_y1 > abs_y2:\n",
    "        abs_y1, abs_y2 = abs_y2, abs_y1\n",
    "\n",
    "      # Draw the bounding box\n",
    "      draw.rectangle(\n",
    "          ((abs_x1, abs_y1), (abs_x2, abs_y2)), outline=color, width=4\n",
    "      )\n",
    "\n",
    "      # Draw the text\n",
    "      if \"label\" in bounding_box:\n",
    "        draw.text((abs_x1 + 8, abs_y1 + 6), bounding_box[\"label\"], fill=color, font=font)\n",
    "\n",
    "    # Display the image\n",
    "    img.show()\n",
    "\n",
    "\n",
    "def plot_points(im, text, input_width, input_height):\n",
    "  img = im\n",
    "  width, height = img.size\n",
    "  draw = ImageDraw.Draw(img)\n",
    "  colors = [\n",
    "    'red', 'green', 'blue', 'yellow', 'orange', 'pink', 'purple', 'brown', 'gray',\n",
    "    'beige', 'turquoise', 'cyan', 'magenta', 'lime', 'navy', 'maroon', 'teal',\n",
    "    'olive', 'coral', 'lavender', 'violet', 'gold', 'silver',\n",
    "  ] + additional_colors\n",
    "  xml_text = text.replace('```xml', '')\n",
    "  xml_text = xml_text.replace('```', '')\n",
    "  data = decode_xml_points(xml_text)\n",
    "  if data is None:\n",
    "    img.show()\n",
    "    return\n",
    "  points = data['points']\n",
    "  description = data['phrase']\n",
    "\n",
    "  font = ImageFont.truetype(\"NotoSansCJK-Regular.ttc\", size=14)\n",
    "\n",
    "  for i, point in enumerate(points):\n",
    "    color = colors[i % len(colors)]\n",
    "    abs_x1 = int(point[0])/input_width * width\n",
    "    abs_y1 = int(point[1])/input_height * height\n",
    "    radius = 2\n",
    "    draw.ellipse([(abs_x1 - radius, abs_y1 - radius), (abs_x1 + radius, abs_y1 + radius)], fill=color)\n",
    "    draw.text((abs_x1 + 8, abs_y1 + 6), description, fill=color, font=font)\n",
    "  \n",
    "  img.show()\n",
    "  \n",
    "\n",
    "# @title Parsing JSON output\n",
    "def parse_json(json_output):\n",
    "    # Parsing out the markdown fencing\n",
    "    lines = json_output.splitlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        if line == \"```json\":\n",
    "            json_output = \"\\n\".join(lines[i+1:])  # Remove everything before \"```json\"\n",
    "            json_output = json_output.split(\"```\")[0]  # Remove everything after the closing \"```\"\n",
    "            break  # Exit the loop once \"```json\" is found\n",
    "    return json_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f554b4",
   "metadata": {},
   "source": [
    "Load model and processors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e829b782-0be7-4bc6-a576-6b815323376e",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-01-29T11:51:26.514720Z",
     "iopub.status.busy": "2025-01-29T11:51:26.514049Z",
     "iopub.status.idle": "2025-01-29T11:51:55.411363Z",
     "shell.execute_reply": "2025-01-29T11:51:55.410649Z",
     "shell.execute_reply.started": "2025-01-29T11:51:26.514696Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "model_path = \"/root/ui-agent-qwenvl/env/Qwen2.5-VL-3B-Instruct\"\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(model_path, torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\",device_map=\"auto\")\n",
    "processor = AutoProcessor.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937abf0d",
   "metadata": {},
   "source": [
    "Load inference function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfa50d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-29T11:52:13.792848Z",
     "iopub.status.busy": "2025-01-29T11:52:13.792221Z",
     "iopub.status.idle": "2025-01-29T11:52:13.798997Z",
     "shell.execute_reply": "2025-01-29T11:52:13.798438Z",
     "shell.execute_reply.started": "2025-01-29T11:52:13.792820Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inference(img_url, prompt, system_prompt=\"You are a helpful assistant\", max_new_tokens=1024):\n",
    "  image = Image.open(img_url)\n",
    "  messages = [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": system_prompt\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": prompt\n",
    "        },\n",
    "        {\n",
    "          \"image\": img_url\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "  text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "  print(\"input:\\n\",text)\n",
    "  inputs = processor(text=[text], images=[image], padding=True, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "  output_ids = model.generate(**inputs, max_new_tokens=1024)\n",
    "  generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n",
    "  output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "  print(\"output:\\n\",output_text[0])\n",
    "\n",
    "  input_height = inputs['image_grid_thw'][0][1]*14\n",
    "  input_width = inputs['image_grid_thw'][0][2]*14\n",
    "\n",
    "  return output_text[0], input_height, input_width"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4f1883-6fc9-425b-aea4-d26298bc8551",
   "metadata": {},
   "source": [
    "inference function with API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6060f970-1c96-48e6-af51-cf0cf3bd00bc",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-01-29T11:52:16.378437Z",
     "iopub.status.busy": "2025-01-29T11:52:16.377899Z",
     "iopub.status.idle": "2025-01-29T11:52:16.384573Z",
     "shell.execute_reply": "2025-01-29T11:52:16.383930Z",
     "shell.execute_reply.started": "2025-01-29T11:52:16.378415Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from openai import OpenAI\n",
    "# import os\n",
    "# import base64\n",
    "# #  base 64 编码格式\n",
    "# def encode_image(image_path):\n",
    "#     with open(image_path, \"rb\") as image_file:\n",
    "#         return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "# # @title inference function with API\n",
    "# def inference_with_api(image_path, prompt, sys_prompt=\"You are a helpful assistant.\", model_id=\"qwen2.5-vl-72b-instruct\", min_pixels=512*28*28, max_pixels=2048*28*28):\n",
    "#     base64_image = encode_image(image_path)\n",
    "#     client = OpenAI(\n",
    "#         #If the environment variable is not configured, please replace the following line with the Dashscope API Key: api_key=\"sk-xxx\".\n",
    "#         api_key=os.getenv('DASHSCOPE_API_KEY'),\n",
    "#         base_url=\"https://dashscope-intl.aliyuncs.com/compatible-mode/v1\",\n",
    "#     )\n",
    "\n",
    "\n",
    "#     messages=[\n",
    "#         {\n",
    "#             \"role\": \"system\",\n",
    "#             \"content\": [{\"type\":\"text\",\"text\": sys_prompt}]},\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": [\n",
    "#                 {\n",
    "#                     \"type\": \"image_url\",\n",
    "#                     \"min_pixels\": min_pixels,\n",
    "#                     \"max_pixels\": max_pixels,\n",
    "#                     # Pass in BASE64 image data. Note that the image format (i.e., image/{format}) must match the Content Type in the list of supported images. \"f\" is the method for string formatting.\n",
    "#                     # PNG image:  f\"data:image/png;base64,{base64_image}\"\n",
    "#                     # JPEG image: f\"data:image/jpeg;base64,{base64_image}\"\n",
    "#                     # WEBP image: f\"data:image/webp;base64,{base64_image}\"\n",
    "#                     \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},\n",
    "#                 },\n",
    "#                 {\"type\": \"text\", \"text\": prompt},\n",
    "#             ],\n",
    "#         }\n",
    "#     ]\n",
    "#     completion = client.chat.completions.create(\n",
    "#         model = model_id,\n",
    "#         messages = messages,\n",
    "       \n",
    "#     )\n",
    "#     return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c340c27",
   "metadata": {},
   "source": [
    "#### 1. Detect certain object in the image\n",
    "\n",
    "Let's start with a simple scenario where we want to locate certain objects in an image.\n",
    "\n",
    "Besides, we can further prompt the model to describe their unique characteristics or features by explicitly giving that order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e969b5-de9f-4efc-b8ae-a95ca441d639",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-01-29T12:19:03.259891Z",
     "iopub.status.busy": "2025-01-29T12:19:03.259307Z",
     "iopub.status.idle": "2025-01-29T12:19:17.272268Z",
     "shell.execute_reply": "2025-01-29T12:19:17.271760Z",
     "shell.execute_reply.started": "2025-01-29T12:19:03.259862Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# image_path = \"./assets/spatial_understanding/cakes.png\"\n",
    "\n",
    "\n",
    "# ## Use a local HuggingFace model to inference.\n",
    "# # prompt in chinese\n",
    "# prompt = \"框出每一个小蛋糕的位置，以json格式输出所有的坐标\"\n",
    "# # prompt in english\n",
    "# prompt = \"Outline the position of each small cake and output all the coordinates in JSON format.\"\n",
    "# response, input_height, input_width = inference(image_path, prompt)\n",
    "\n",
    "# image = Image.open(image_path)\n",
    "# print(image.size)\n",
    "# image.thumbnail([640,640], Image.Resampling.LANCZOS)\n",
    "# plot_bounding_boxes(image,response,input_width,input_height)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ## Use an API-based approach to inference. Apply API key here: https://bailian.console.alibabacloud.com/?apiKey=1\n",
    "# # from qwen_vl_utils import smart_resize\n",
    "# # os.environ['DASHSCOPE_API_KEY'] = 'your_api_key_here' \n",
    "# # min_pixels = 512*28*28\n",
    "# # max_pixels = 2048*28*28\n",
    "# # image = Image.open(image_path)\n",
    "# # width, height = image.size\n",
    "# # input_height,input_width = smart_resize(height,width,min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "# # response = inference_with_api(image_path, prompt, min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "# # plot_bounding_boxes(image, response, input_width, input_height)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83111604",
   "metadata": {},
   "source": [
    "#### 2. Detect a specific object using descriptions\n",
    "\n",
    "Further, you can search for a specific object by using a short phrase or sentence to describe it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f34464d-d7c7-4dbe-81d1-b811eceb9c5e",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-01-29T12:20:58.885718Z",
     "iopub.status.busy": "2025-01-29T12:20:58.885124Z",
     "iopub.status.idle": "2025-01-29T12:21:00.739805Z",
     "shell.execute_reply": "2025-01-29T12:21:00.739195Z",
     "shell.execute_reply.started": "2025-01-29T12:20:58.885697Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_path = \"assets/document_parsing/diffcl-v34_page_5.jpg\"\n",
    "\n",
    "# prompt in chinses\n",
    "prompt = \"请定位图片中所有表格的位置，以JSON格式输出其bbox坐标\"\n",
    "# prompt in english\n",
    "# prompt = \"Locate the top right brown cake, output its bbox coordinates using JSON format.\"\n",
    "\n",
    "## Use a local HuggingFace model to inference.\n",
    "response, input_height, input_width = inference(image_path, prompt)\n",
    "\n",
    "image = Image.open(image_path)\n",
    "image.thumbnail([640,640], Image.Resampling.LANCZOS)\n",
    "plot_bounding_boxes(image,response,input_width,input_height)\n",
    "\n",
    "## Use an API-based approach to inference. Apply API key here: https://bailian.console.alibabacloud.com/?apiKey=1\n",
    "# from qwen_vl_utils import smart_resize\n",
    "# os.environ['DASHSCOPE_API_KEY'] = 'your_api_key_here' \n",
    "# min_pixels = 512*28*28\n",
    "# max_pixels = 2048*28*28\n",
    "# image = Image.open(image_path)\n",
    "# width, height = image.size\n",
    "# input_height,input_width = smart_resize(height,width,min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "# response = inference_with_api(image_path, prompt, min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "# plot_bounding_boxes(image, response, input_width, input_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851bf964",
   "metadata": {},
   "source": [
    "#### 3. Point to certain objects in xml format\n",
    "\n",
    "In addition to the above mentioned bbox format [x1, y1, x2, y2], Qwen2.5-VL also supports point-based grounding. You can point to a specific object and the model is trained to output xml-style results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c900f9dd",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-01-29T12:22:08.238982Z",
     "iopub.status.busy": "2025-01-29T12:22:08.238698Z",
     "iopub.status.idle": "2025-01-29T12:22:12.129376Z",
     "shell.execute_reply": "2025-01-29T12:22:12.128799Z",
     "shell.execute_reply.started": "2025-01-29T12:22:08.238962Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# image_path = \"./assets/spatial_understanding/cakes.png\"\n",
    "\n",
    "# # prompt in chinese\n",
    "# prompt = \"以点的形式定位图中桌子远处的擀面杖，以XML格式输出其坐标\"\n",
    "# # prompt in english\n",
    "# prompt = \"point to the rolling pin on the far side of the table, output its coordinates in XML format <points x y>object</points>\"\n",
    "\n",
    "# ## Use a local HuggingFace model to inference.\n",
    "# response, input_height, input_width = inference(image_path, prompt)\n",
    "\n",
    "# image = Image.open(image_path)\n",
    "# image.thumbnail([640,640], Image.Resampling.LANCZOS)\n",
    "# plot_points(image, response, input_width, input_height)\n",
    "\n",
    "# ## Use an API-based approach to inference. Apply API key here: https://bailian.console.alibabacloud.com/?apiKey=1\n",
    "# # from qwen_vl_utils import smart_resize\n",
    "# # os.environ['DASHSCOPE_API_KEY'] = 'your_api_key_here' \n",
    "# # min_pixels = 512*28*28\n",
    "# # max_pixels = 2048*28*28\n",
    "# # image = Image.open(image_path)\n",
    "# # width, height = image.size\n",
    "# # input_height,input_width = smart_resize(height,width,min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "# # response = inference_with_api(image_path, prompt, min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "# # plot_points(image, response, input_width, input_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f55c1c",
   "metadata": {},
   "source": [
    "#### 4. Reasoning capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5352ad",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-01-29T12:33:12.089077Z",
     "iopub.status.busy": "2025-01-29T12:33:12.088520Z",
     "iopub.status.idle": "2025-01-29T12:33:18.017727Z",
     "shell.execute_reply": "2025-01-29T12:33:18.017102Z",
     "shell.execute_reply.started": "2025-01-29T12:33:12.089055Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# image_path = \"./assets/spatial_understanding/Origamis.jpg\"\n",
    "\n",
    "# # prompt in chinese\n",
    "# prompt = \"框出图中纸狐狸的影子，以json格式输出其bbox坐标\"\n",
    "# # prompt in english\n",
    "# prompt = \"Locate the shadow of the paper fox, report the bbox coordinates in JSON format.\"\n",
    "\n",
    "# ## Use a local HuggingFace model to inference.\n",
    "# response, input_height, input_width = inference(image_path, prompt)\n",
    "\n",
    "# image = Image.open(image_path)\n",
    "# image.thumbnail([640,640], Image.Resampling.LANCZOS)\n",
    "# plot_bounding_boxes(image, response, input_width, input_height)\n",
    "\n",
    "# ## Use an API-based approach to inference. Apply API key here: https://bailian.console.alibabacloud.com/?apiKey=1\n",
    "# # from qwen_vl_utils import smart_resize\n",
    "# # os.environ['DASHSCOPE_API_KEY'] = 'your_api_key_here' \n",
    "# # min_pixels = 512*28*28\n",
    "# # max_pixels = 2048*28*28\n",
    "# # image = Image.open(image_path)\n",
    "# # width, height = image.size\n",
    "# # input_height,input_width = smart_resize(height,width,min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "# # response = inference_with_api(image_path, prompt, min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "# # plot_bounding_boxes(image, response, input_width, input_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3286859",
   "metadata": {},
   "source": [
    "#### 5. Understand relationships across different instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9e934f",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-01-29T12:24:36.427415Z",
     "iopub.status.busy": "2025-01-29T12:24:36.427110Z",
     "iopub.status.idle": "2025-01-29T12:24:38.098639Z",
     "shell.execute_reply": "2025-01-29T12:24:38.097988Z",
     "shell.execute_reply.started": "2025-01-29T12:24:36.427395Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# image_path = \"./assets/spatial_understanding/cartoon_brave_person.jpeg\"\n",
    "\n",
    "# # prompt in chinese\n",
    "# prompt = \"框出图中见义勇为的人，以json格式输出其bbox坐标\"\n",
    "# # prompt in english\n",
    "# prompt = \"Locate the person who act bravely, report the bbox coordinates in JSON format.\"\n",
    "\n",
    "# ## Use a local HuggingFace model to inference.\n",
    "# response, input_height, input_width = inference(image_path, prompt)\n",
    "\n",
    "# image = Image.open(image_path)\n",
    "# image.thumbnail([640,640], Image.Resampling.LANCZOS)\n",
    "# plot_bounding_boxes(image, response, input_width, input_height)\n",
    "\n",
    "\n",
    "# ## Use an API-based approach to inference. Apply API key here: https://bailian.console.alibabacloud.com/?apiKey=1\n",
    "# # from qwen_vl_utils import smart_resize\n",
    "# # os.environ['DASHSCOPE_API_KEY'] = 'your_api_key_here' \n",
    "# # min_pixels = 512*28*28\n",
    "# # max_pixels = 2048*28*28\n",
    "# # image = Image.open(image_path)\n",
    "# # width, height = image.size\n",
    "# # input_height,input_width = smart_resize(height,width,min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "# # response = inference_with_api(image_path, prompt, min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "# # plot_bounding_boxes(image, response, input_width, input_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66761508",
   "metadata": {},
   "source": [
    "#### 6. Find a special instance with unique characteristic (color, location, utility, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd74c94",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# url = \"./assets/spatial_understanding/multiple_items.png\"\n",
    "\n",
    "# # prompt in chinese\n",
    "# prompt = \"如果太阳很刺眼，我应该用这张图中的什么物品，框出该物品在图中的bbox坐标，并以json格式输出\"\n",
    "# # prompt in english\n",
    "# prompt = \"If the sun is very glaring, which item in this image should I use? Please locate it in the image with its bbox coordinates and its name and output in JSON format.\"\n",
    "\n",
    "# ## Use a local HuggingFace model to inference.\n",
    "# response, input_height, input_width = inference(url, prompt)\n",
    "\n",
    "# image = Image.open(url)\n",
    "# image.thumbnail([640,640], Image.Resampling.LANCZOS)\n",
    "# plot_bounding_boxes(image, response, input_width, input_height)\n",
    "\n",
    "\n",
    "# ## Use an API-based approach to inference. Apply API key here: https://bailian.console.alibabacloud.com/?apiKey=1\n",
    "# # from qwen_vl_utils import smart_resize\n",
    "# # os.environ['DASHSCOPE_API_KEY'] = 'your_api_key_here' \n",
    "# # min_pixels = 512*28*28\n",
    "# # max_pixels = 2048*28*28\n",
    "# # image = Image.open(image_path)\n",
    "# # width, height = image.size\n",
    "# # input_height,input_width = smart_resize(height,width,min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "# # response = inference_with_api(image_path, prompt, min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "# # plot_bounding_boxes(image, response, input_width, input_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85ce363",
   "metadata": {},
   "source": [
    "#### 7. Use Qwen2.5-VL grounding capabilities to help counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44595cbc",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-01-29T12:30:05.347521Z",
     "iopub.status.busy": "2025-01-29T12:30:05.346923Z",
     "iopub.status.idle": "2025-01-29T12:30:19.636437Z",
     "shell.execute_reply": "2025-01-29T12:30:19.635236Z",
     "shell.execute_reply.started": "2025-01-29T12:30:05.347495Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# image_path = \"./assets/spatial_understanding/multiple_items.png\"\n",
    "\n",
    "# # prompt in chinese\n",
    "# prompt = \"请以JSON格式输出图中所有物体bbox的坐标以及它们的名字，然后基于检测结果回答以下问题：图中物体的数目是多少？\"\n",
    "# # prompt in english\n",
    "# prompt = \"Please first output bbox coordinates and names of every item in this image in JSON format, and then answer how many items are there in the image.\"\n",
    "\n",
    "# ## Use a local HuggingFace model to inference.\n",
    "# response, input_height, input_width = inference(image_path, prompt)\n",
    "\n",
    "# image = Image.open(image_path)\n",
    "# image.thumbnail([640,640], Image.Resampling.LANCZOS)\n",
    "# plot_bounding_boxes(image,response,input_width,input_height)\n",
    "\n",
    "# # # Use an API-based approach to inference. Apply API key here: https://bailian.console.alibabacloud.com/?apiKey=1\n",
    "# # from qwen_vl_utils import smart_resize\n",
    "# # os.environ['DASHSCOPE_API_KEY'] = 'your_api_key_here' \n",
    "# # min_pixels = 512*28*28\n",
    "# # max_pixels = 2048*28*28\n",
    "# # image = Image.open(image_path)\n",
    "# # width, height = image.size\n",
    "# # input_height,input_width = smart_resize(height,width,min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "# # response = inference_with_api(image_path, prompt, min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "# # plot_bounding_boxes(image, response, input_width, input_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fca8fc6",
   "metadata": {},
   "source": [
    "#### 8. spatial understanding with designed system prompt\n",
    "The above usage is based on the default system prompt. You can also change the system prompt to obtain other output format like plain text.\n",
    "Qwen2.5-VL now support these formats:\n",
    "* bbox-format: JSON\n",
    "\n",
    "`{\"bbox_2d\": [x1, y1, x2, y2], \"label\": \"object name/description\"}`\n",
    "\n",
    "* bbox-format: plain text\n",
    "\n",
    "`x1,y1,x2,y2 object_name/description`\n",
    "\n",
    "* point-format: XML\n",
    "\n",
    "`<points x y>object_name/description</points>`\n",
    "\n",
    "* point-format: JSON\n",
    "\n",
    "`{\"point_2d\": [x, y], \"label\": \"object name/description\"}`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1730b6b",
   "metadata": {},
   "source": [
    "Change your system prompt to use plain text as output format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2de12a4",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-01-29T12:35:16.897325Z",
     "iopub.status.busy": "2025-01-29T12:35:16.896787Z",
     "iopub.status.idle": "2025-01-29T12:35:17.798390Z",
     "shell.execute_reply": "2025-01-29T12:35:17.797701Z",
     "shell.execute_reply.started": "2025-01-29T12:35:16.897301Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# image_path = \"./assets/spatial_understanding/cakes.png\"\n",
    "# image = Image.open(image_path)\n",
    "# system_prompt = \"As an AI assistant, you specialize in accurate image object detection, delivering coordinates in plain text format 'x1,y1,x2,y2 object'.\"\n",
    "# prompt = \"find all cakes\"\n",
    "# response, input_height, input_width = inference(image_path, prompt, system_prompt=system_prompt)\n",
    "\n",
    "\n",
    "\n",
    "# ## Use an API-based approach to inference. Apply API key here: https://bailian.console.alibabacloud.com/?apiKey=1\n",
    "# # from qwen_vl_utils import smart_resize\n",
    "# # os.environ['DASHSCOPE_API_KEY'] = 'your_api_key_here' \n",
    "# # response = inference_with_api(image_path, prompt, sys_prompt=system_prompt)\n",
    "# # print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b7a76a-550f-4c5f-a354-8901c042efcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ui-agent-qwenvl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
