{
  "source_file": "tmp\\v9.pdf",
  "output_file": "tmp\\v9_bbox.pdf",
  "creation_time": "2025-07-08 23:46:53",
  "total_pages": 14,
  "summary": {
    "total_text_blocks": 129,
    "total_images": 4,
    "total_tables": 6,
    "refined_tables": 6
  },
  "pages": {
    "1": {
      "page_number": 1,
      "elements": [
        {
          "type": "text",
          "bbox": [
            135.48399353027344,
            114.58596801757812,
            480.0221862792969,
            164.79812622070312
          ],
          "index": 0,
          "content": "GenFRC: Generative Feature Replay and Calibration for Non-Exemplar Class-Incremental Learning"
        },
        {
          "type": "text",
          "bbox": [
            258.9019775390625,
            188.50987243652344,
            356.4657897949219,
            198.47247314453125
          ],
          "index": 0,
          "content": "Anonymous Author(s)"
        },
        {
          "type": "text",
          "bbox": [
            163.11097717285156,
            226.15843200683594,
            452.3044128417969,
            410.46759033203125
          ],
          "index": 0,
          "content": "Abstract.  Image classification is a fundamental computer vision task with broad practical applications. However, real-world visual systems in- herently learn in an incremental manner as category dist"
        },
        {
          "type": "text",
          "bbox": [
            163.11097717285156,
            405.4111328125,
            452.2774658203125,
            442.986572265625
          ],
          "index": 0,
          "content": "Keywords:  Incremental Learning  ·  Lifelong Learning  ·  Catastrophic Forgetting."
        },
        {
          "type": "text",
          "bbox": [
            134.76498413085938,
            463.72735595703125,
            228.995849609375,
            475.68255615234375
          ],
          "index": 0,
          "content": "1 Introduction"
        },
        {
          "type": "text",
          "bbox": [
            134.76498413085938,
            487.7636413574219,
            480.6564636230469,
            665.0982055664062
          ],
          "index": 0,
          "content": "Image classification is a fundamental computer vision task with broad practical applications [2,14,41]. However, real-world visual systems must learn incremen- tally, constantly adapting to new catego"
        }
      ]
    },
    "2": {
      "page_number": 2,
      "elements": [
        {
          "type": "text",
          "bbox": [
            134.76499938964844,
            92.88749694824219,
            215.2294921875,
            101.85389709472656
          ],
          "index": 0,
          "content": "2 Anonymous"
        },
        {
          "type": "text",
          "bbox": [
            134.76499938964844,
            118.01194763183594,
            480.65643310546875,
            223.61663818359375
          ],
          "index": 0,
          "content": "overwrite discriminative pathways for old classes, gradually weakening gradient signals tied to earlier tasks [7]. The resulting imbalance between acquiring new knowledge and retaining old representat"
        },
        {
          "type": "text",
          "bbox": [
            134.76499938964844,
            231.97703552246094,
            480.6366271972656,
            349.5365905761719
          ],
          "index": 0,
          "content": "Recent NECIL methods primarily develop along three technical directions: generative replay, parameter regularization, and architecture optimization. Gen- erative replay methods [25] preserve old knowl"
        },
        {
          "type": "text",
          "bbox": [
            134.76499938964844,
            357.8970031738281,
            480.65655517578125,
            511.3214416503906
          ],
          "index": 0,
          "content": "To mitigate catastrophic forgetting, we propose synthesizing representative pseudo-features of previously learned classes and adaptively calibrating them using the feature representations of newly enc"
        },
        {
          "type": "text",
          "bbox": [
            138.9709930419922,
            546.7578735351562,
            480.6383056640625,
            580.6314697265625
          ],
          "index": 0,
          "content": "1. We propose Hybrid-Conditional Variational Replay (HCVR) to generate rich and diverse pseudo-features that effectively mitigate forgetting at the repre- sentation level."
        },
        {
          "type": "text",
          "bbox": [
            138.9709930419922,
            588.9918823242188,
            480.63616943359375,
            622.8654174804688
          ],
          "index": 0,
          "content": "2. We propose Fourier Kernel Alignment Module (FKAM) to preserve distri- butional coherence across tasks, reducing feature drift during incremental updates."
        },
        {
          "type": "text",
          "bbox": [
            138.9709930419922,
            631.225830078125,
            480.6262512207031,
            665.0984497070312
          ],
          "index": 0,
          "content": "3. We conduct comprehensive experiments across multiple class-incremental learning benchmarks, demonstrating that GenFRC consistently outperforms state-of-the-art approaches."
        }
      ]
    },
    "3": {
      "page_number": 3,
      "elements": [
        {
          "type": "text",
          "bbox": [
            222.78199768066406,
            92.88749694824219,
            480.6198425292969,
            101.85389709472656
          ],
          "index": 0,
          "content": "GenFRC for Non-Exemplar Class-Incremental Learning 3"
        },
        {
          "type": "text",
          "bbox": [
            134.76499938964844,
            116.45466613769531,
            236.81454467773438,
            128.4098663330078
          ],
          "index": 0,
          "content": "2 Related Work"
        },
        {
          "type": "text",
          "bbox": [
            134.76499938964844,
            143.2229461669922,
            330.6994323730469,
            153.185546875
          ],
          "index": 0,
          "content": "2.1 Class-Incremental Learning (CIL)"
        },
        {
          "type": "text",
          "bbox": [
            134.76499938964844,
            164.44895935058594,
            480.6663818359375,
            293.9636535644531
          ],
          "index": 0,
          "content": "CIL aims to incorporate new categories sequentially while retaining prior knowl- edge. To tackle the core challenge of catastrophic forgetting, existing methods are typically classified into three cat"
        },
        {
          "type": "text",
          "bbox": [
            134.76499938964844,
            315.1890563964844,
            306.0221252441406,
            325.1516418457031
          ],
          "index": 0,
          "content": "2.2 Non-Exemplar CIL (NECIL)"
        },
        {
          "type": "text",
          "bbox": [
            134.76499938964844,
            336.4150695800781,
            484.43231201171875,
            501.79449462890625
          ],
          "index": 0,
          "content": "In privacy-sensitive scenarios where storing historical samples is infeasible, NECIL addresses the CIL problem through alternative mechanisms such as knowledge distillation, generative replay and para"
        },
        {
          "type": "text",
          "bbox": [
            134.76499938964844,
            523.45556640625,
            232.1520233154297,
            535.4107666015625
          ],
          "index": 0,
          "content": "3 Methodology"
        },
        {
          "type": "text",
          "bbox": [
            134.76499938964844,
            550.2239379882812,
            207.92037963867188,
            560.1865234375
          ],
          "index": 0,
          "content": "3.1 Overview"
        },
        {
          "type": "text",
          "bbox": [
            134.76499938964844,
            571.4498901367188,
            482.2604675292969,
            665.0985107421875
          ],
          "index": 0,
          "content": "Fig. 1 illustrates the GenFRC method proposed for NECIL. It comprises two complementary components: (i) Hybrid-Conditional Variational Replay (HCVR), designed to synthesize representative pseudo-featu"
        }
      ]
    },
    "4": {
      "page_number": 4,
      "elements": [
        {
          "type": "text",
          "bbox": [
            134.76499938964844,
            92.88749694824219,
            215.2294921875,
            101.85389709472656
          ],
          "index": 0,
          "content": "4 Anonymous"
        },
        {
          "type": "text",
          "bbox": [
            134.76499938964844,
            235.4159393310547,
            480.6464538574219,
            269.28955078125
          ],
          "index": 0,
          "content": "Fig. 1: Overview of the proposed GenFRC. Hybrid-Conditional Variational Re- play (HCVR) generates old-class features locally, while the Fourier Kernel Align- ment Module (FKAM) aligns feature distribu"
        },
        {
          "type": "text",
          "bbox": [
            134.76499938964844,
            292.5339660644531,
            454.5544128417969,
            314.4515380859375
          ],
          "index": 0,
          "content": "3.2 Hybrid-Conditional Variational Replay (HCVR) for Local Feature Replay"
        },
        {
          "type": "text",
          "bbox": [
            134.76498413085938,
            321.39495849609375,
            480.61663818359375,
            379.1784973144531
          ],
          "index": 0,
          "content": "In non-exemplar setups, catastrophic forgetting arises from the loss of class dis- criminability over time. HCVR addresses this issue within a probabilistic gen- erative method that synthesises repres"
        },
        {
          "type": "text",
          "bbox": [
            134.76498413085938,
            395.9548034667969,
            480.6358642578125,
            441.9124755859375
          ],
          "index": 0,
          "content": "Probabilistic Feature Replay with Hybrid Conditioning.  Let  p θ ( f old  |  c ) be the decoder likelihood of an old-class feature  f old  given the hybrid condition  c . A variational encoder  q ϕ ( "
        },
        {
          "type": "text",
          "bbox": [
            149.11801147460938,
            447.8163757324219,
            480.5932312011719,
            461.4192199707031
          ],
          "index": 0,
          "content": "log  p θ ( f old  |  c )  ≥ E q ϕ [log  p θ ( f old  |  z, c )]  − β D KL ( q ϕ ( z  |  f old , c )  ∥ p ( z )) , (1)"
        },
        {
          "type": "text",
          "bbox": [
            134.76495361328125,
            468.1077880859375,
            480.6281433105469,
            646.9554443359375
          ],
          "index": 0,
          "content": "where  p ( z ) =  N (0 , I ) and  β  is the  β -VAE coefficient that regulates disentangle- ment [13]. The hybrid condition vector  c  integrates three complementary sources of in- formation to enhanc"
        },
        {
          "type": "text",
          "bbox": [
            220.27198791503906,
            655.0076904296875,
            480.5955505371094,
            666.46435546875
          ],
          "index": 0,
          "content": "c  =  W cls  ·  c cls  ⊕ W geo  ·  c geo  ⊕ W syn  ·  c syn , (2)"
        },
        {
          "type": "image",
          "bbox": [
            160.7010040283203,
            115.83392333984375,
            454.66033935546875,
            224.8709716796875
          ],
          "index": 0
        }
      ]
    },
    "5": {
      "page_number": 5,
      "elements": [
        {
          "type": "text",
          "bbox": [
            222.78199768066406,
            92.88749694824219,
            480.6198425292969,
            101.85389709472656
          ],
          "index": 0,
          "content": "GenFRC for Non-Exemplar Class-Incremental Learning 5"
        },
        {
          "type": "text",
          "bbox": [
            134.76499938964844,
            117.8828353881836,
            480.63653564453125,
            151.88555908203125
          ],
          "index": 0,
          "content": "where  W {·}  are learnable projection matrices and  ⊕ denotes concatenation. This design preserves distinct information channels while enabling adaptive weighting of different condition types."
        },
        {
          "type": "text",
          "bbox": [
            134.76499938964844,
            174.12684631347656,
            480.60638427734375,
            196.173583984375
          ],
          "index": 0,
          "content": "Variational Inference and Feature Reconstruction.  The encoder  q ϕ ( z | f, c ) maps hybrid-conditioned features to latent parameters:"
        },
        {
          "type": "text",
          "bbox": [
            187.93402099609375,
            206.91481018066406,
            413.9710693359375,
            225.7386474609375
          ],
          "index": 0,
          "content": "µ,  log  σ 2   = MLP([ f old ;  c ]) , z  =  µ  +  ϵ  ⊙ exp \u0012 log  σ 2"
        },
        {
          "type": "text",
          "bbox": [
            399.45001220703125,
            221.76792907714844,
            404.4313049316406,
            231.73052978515625
          ],
          "index": 0,
          "content": "2"
        },
        {
          "type": "text",
          "bbox": [
            415.6600036621094,
            208.0503692626953,
            480.593994140625,
            224.89654541015625
          ],
          "index": 0,
          "content": "\u0013 . (3)"
        },
        {
          "type": "text",
          "bbox": [
            134.76498413085938,
            243.67884826660156,
            375.8843994140625,
            254.61163330078125
          ],
          "index": 0,
          "content": "The decoder  p θ ( f old | z, c ) reconstructs features through:"
        },
        {
          "type": "text",
          "bbox": [
            206.19998168945312,
            265.15399169921875,
            480.6004638671875,
            279.5556640625
          ],
          "index": 0,
          "content": "ˆ f old  = MLP([ z ;  c ]) +  ϵ obs , ϵ obs  ∼N (0 , σ 2 obs I ) , (4)"
        },
        {
          "type": "text",
          "bbox": [
            134.76498413085938,
            290.67901611328125,
            480.6248779296875,
            312.5965881347656
          ],
          "index": 0,
          "content": "where  σ obs  models reconstruction uncertainty. The reconstruction loss in Eq. (1) thus becomes:"
        },
        {
          "type": "text",
          "bbox": [
            204.27499389648438,
            320.99102783203125,
            480.5989074707031,
            346.87957763671875
          ],
          "index": 0,
          "content": "log  p θ ( f old | z, c ) =  − 1 2 σ 2 obs ∥ f old  − ˆ f old ∥ 2 2   + const. (5)"
        },
        {
          "type": "text",
          "bbox": [
            134.76502990722656,
            365.8629150390625,
            480.5968017578125,
            399.7354736328125
          ],
          "index": 0,
          "content": "Dynamic Replay via Feature Manifold Interpolation.  During incremental stages  t , synthetic features are generated through latent space traversal condi- tioned on historical class manifolds:"
        },
        {
          "type": "text",
          "bbox": [
            215.71702575683594,
            408.4267578125,
            480.6102294921875,
            425.6374816894531
          ],
          "index": 0,
          "content": "ˆ f   ( i ) old   = Decoder \u0000 z ( i ) , c ( i ) \u0001 , z ( i )   ∼N (0 , I ) , (6)"
        },
        {
          "type": "text",
          "bbox": [
            134.76507568359375,
            434.88287353515625,
            480.60882568359375,
            460.41644287109375
          ],
          "index": 0,
          "content": "where  c ( i )   =  c ( i ) cls ⊕ c ( i ) geo ⊕ c ( i ) syn . The mix components  c ( i ) syn   are dynamically adjusted based on the current task’s class distribution:"
        },
        {
          "type": "text",
          "bbox": [
            221.14012145996094,
            478.9996643066406,
            248.21002197265625,
            493.2024230957031
          ],
          "index": 0,
          "content": "c ( i ) syn   ="
        },
        {
          "type": "text",
          "bbox": [
            250.97512817382812,
            470.4876403808594,
            266.80584716796875,
            488.4388427734375
          ],
          "index": 0,
          "content": "K old X"
        },
        {
          "type": "text",
          "bbox": [
            252.244140625,
            477.9376525878906,
            480.5949401855469,
            501.8414611816406
          ],
          "index": 0,
          "content": "j =1 π j c ( j ) cls , π  ∼ Dirichlet( α syn ) , (7)"
        },
        {
          "type": "text",
          "bbox": [
            134.76515197753906,
            513.7398071289062,
            480.6314697265625,
            560.056396484375
          ],
          "index": 0,
          "content": "where  K old  is the number of old classes and  α syn  controls interpolation diversity. Let  F old  and  F syn  denote the true and synthetic feature distributions. Un- der Lipschitz continuity assum"
        },
        {
          "type": "text",
          "bbox": [
            217.59115600585938,
            570.8132934570312,
            480.5957946777344,
            583.79345703125
          ],
          "index": 0,
          "content": "W 2 ( F old ,  F syn )  ≤ L dec E [ ∥ z  − z ′ ∥ 2 ] +  σ obs , (8)"
        },
        {
          "type": "text",
          "bbox": [
            134.76513671875,
            592.2698364257812,
            480.65673828125,
            642.202392578125
          ],
          "index": 0,
          "content": "where  L dec  is the decoder’s Lipschitz constant and  z ′   is the latent variable for real features, following [1, 30]. Minimizing Eq. (1) directly reduces this bound through the reconstruction term"
        },
        {
          "type": "text",
          "bbox": [
            176.38113403320312,
            652.5068359375,
            480.61846923828125,
            667.5614013671875
          ],
          "index": 0,
          "content": "L CVAE  =  ∥ f old  − ˆ f old ∥ 2 2   +  β CVAE   ·  D KL   ( q ( z  |  f, c )  ∥N (0 , I ))  . (9)"
        }
      ]
    },
    "6": {
      "page_number": 6,
      "elements": [
        {
          "type": "text",
          "bbox": [
            134.76499938964844,
            92.88749694824219,
            215.2294921875,
            101.85389709472656
          ],
          "index": 0,
          "content": "6 Anonymous"
        },
        {
          "type": "text",
          "bbox": [
            134.76499938964844,
            118.01194763183594,
            477.916748046875,
            139.9295654296875
          ],
          "index": 0,
          "content": "3.3 Fourier Kernel Alignment Module (FKAM) for Global Feature Calibration"
        },
        {
          "type": "text",
          "bbox": [
            134.76498413085938,
            150.0919647216797,
            480.646484375,
            195.92059326171875
          ],
          "index": 0,
          "content": "FKAM addresses feature drift by aligning feature distributions between incre- mental tasks using MMD metric, approximated efficiently with RFF. An adap- tive regularization schedule balancing stabilit"
        },
        {
          "type": "text",
          "bbox": [
            134.76498413085938,
            216.04600524902344,
            480.6665344238281,
            261.8736572265625
          ],
          "index": 0,
          "content": "Kernel-Induced Feature Space Alignment.  The MMD metric quantifies distributional discrepancy within a Reproducing Kernel Hilbert Space (RKHS). Let  P  and  Q  denote feature distributions of old and "
        },
        {
          "type": "text",
          "bbox": [
            148.11898803710938,
            271.5495910644531,
            480.5935363769531,
            290.1517639160156
          ],
          "index": 0,
          "content": "MMD 2 ( P, Q ) =  E x,x ′ ∼ P  [ k ( x, x ′ )] +  E y,y ′ ∼ Q [ k ( y, y ′ )]  − 2 E x ∼ P y ∼ Q [ k ( x, y )] , (10)"
        },
        {
          "type": "text",
          "bbox": [
            134.76504516601562,
            300.5790100097656,
            480.6368103027344,
            334.5816955566406
          ],
          "index": 0,
          "content": "where  k ( · ,  · ) is typically the Radial Basis Function (RBF) kernel  k ( x, y ) = exp( − γ ∥ x  − y ∥ 2 ). Direct computation requires  O (( N  +  M ) 2 ) operations for  N new and  M  old feature"
        },
        {
          "type": "text",
          "bbox": [
            134.76504516601562,
            354.70611572265625,
            480.606689453125,
            376.62469482421875
          ],
          "index": 0,
          "content": "RFF Approximation with Error Bounds.  By Bochner’s theorem, any sta- tionary kernel admits the following expression:"
        },
        {
          "type": "text",
          "bbox": [
            205.1390380859375,
            379.8030090332031,
            270.5108947753906,
            399.4637451171875
          ],
          "index": 0,
          "content": "k ( x, y ) =  E ω [ √"
        },
        {
          "type": "text",
          "bbox": [
            270.5140075683594,
            379.80181884765625,
            341.97186279296875,
            398.62152099609375
          ],
          "index": 0,
          "content": "2 cos( ω ⊤ x  +  b ) √"
        },
        {
          "type": "text",
          "bbox": [
            341.9750061035156,
            386.7913818359375,
            480.5930480957031,
            398.62152099609375
          ],
          "index": 0,
          "content": "2 cos( ω ⊤ y  +  b )] , (11)"
        },
        {
          "type": "text",
          "bbox": [
            134.7650146484375,
            410.5278015136719,
            480.6663513183594,
            434.0684814453125
          ],
          "index": 0,
          "content": "with  ω  ∼N (0 ,  2 γI ) and  b  ∼ U [0 ,  2 π ]. Using Monte Carlo approximation with D rff  samples, we obtain:"
        },
        {
          "type": "text",
          "bbox": [
            171.8450164794922,
            447.23492431640625,
            290.4995422363281,
            471.612548828125
          ],
          "index": 0,
          "content": "k ( x, y )  ≈ φ ( x ) ⊤ φ ( y ) = 2 D rff"
        },
        {
          "type": "text",
          "bbox": [
            293.8539733886719,
            443.68572998046875,
            308.24993896484375,
            461.63592529296875
          ],
          "index": 0,
          "content": "D rff X"
        },
        {
          "type": "text",
          "bbox": [
            294.5959777832031,
            452.1063232421875,
            480.593505859375,
            475.0385437011719
          ],
          "index": 0,
          "content": "i =1 cos( ω ⊤ i   x  +  b i ) cos( ω ⊤ i   y  +  b i ) . (12)"
        },
        {
          "type": "text",
          "bbox": [
            134.76498413085938,
            478.47174072265625,
            480.6063232421875,
            508.2065124511719
          ],
          "index": 0,
          "content": "The approximation error decays as  O (1 / √ D rff ) [24], ensuring theoretical con- sistency. Substituting Eq. (12) into Eq. (10) yields the RFF-MMD loss:"
        },
        {
          "type": "text",
          "bbox": [
            207.1589813232422,
            534.0988159179688,
            249.3308868408203,
            545.685546875
          ],
          "index": 0,
          "content": "L FKAM  ="
        },
        {
          "type": "text",
          "bbox": [
            252.0959930419922,
            520.9683227539062,
            257.63519287109375,
            560.8189697265625
          ],
          "index": 0,
          "content": ""
        },
        {
          "type": "text",
          "bbox": [
            258.82598876953125,
            527.4889526367188,
            266.8359069824219,
            551.0245361328125
          ],
          "index": 0,
          "content": "1 N"
        },
        {
          "type": "text",
          "bbox": [
            270.77301025390625,
            524.1107788085938,
            285.1689758300781,
            541.8900146484375
          ],
          "index": 0,
          "content": "N X"
        },
        {
          "type": "text",
          "bbox": [
            271.5150146484375,
            527.4889526367188,
            341.33331298828125,
            555.2926025390625
          ],
          "index": 0,
          "content": "i =1 φ ( f   ( i ) new )  − 1"
        },
        {
          "type": "text",
          "bbox": [
            333.4670104980469,
            541.0619506835938,
            343.1307373046875,
            551.0245361328125
          ],
          "index": 0,
          "content": "M"
        },
        {
          "type": "text",
          "bbox": [
            347.0740051269531,
            524.1107788085938,
            361.469970703125,
            541.8900146484375
          ],
          "index": 0,
          "content": "M X"
        },
        {
          "type": "text",
          "bbox": [
            347.3760070800781,
            531.3887329101562,
            393.7664794921875,
            555.2926025390625
          ],
          "index": 0,
          "content": "j =1 φ ( f   ( j ) old )"
        },
        {
          "type": "text",
          "bbox": [
            393.7650146484375,
            520.9683227539062,
            399.3042297363281,
            560.8189697265625
          ],
          "index": 0,
          "content": ""
        },
        {
          "type": "text",
          "bbox": [
            399.3000183105469,
            518.6036987304688,
            403.27508544921875,
            525.5775146484375
          ],
          "index": 0,
          "content": "2"
        },
        {
          "type": "text",
          "bbox": [
            399.3000183105469,
            552.5046997070312,
            403.27508544921875,
            559.478515625
          ],
          "index": 0,
          "content": "2"
        },
        {
          "type": "text",
          "bbox": [
            405.4300231933594,
            534.2279052734375,
            480.59783935546875,
            544.1904907226562
          ],
          "index": 0,
          "content": ". (13)"
        },
        {
          "type": "text",
          "bbox": [
            134.7650146484375,
            568.55078125,
            480.61810302734375,
            590.5975341796875
          ],
          "index": 0,
          "content": "This reduces computational complexity to  O (( N  + M ) D rff ), linear in the number of samples."
        },
        {
          "type": "text",
          "bbox": [
            134.7650146484375,
            610.721923828125,
            480.54681396484375,
            632.6405029296875
          ],
          "index": 0,
          "content": "Adaptive Regularization with Stability-Plasticity Tradeoff.  The regu- larization strength  λ ( t ) is dynamically adjusted during training according to:"
        },
        {
          "type": "text",
          "bbox": [
            215.12200927734375,
            643.769287109375,
            375.30352783203125,
            661.9804077148438
          ],
          "index": 0,
          "content": "λ ( t ) =  λ 0  ·  η ( t ) , η ( t ) = max \u0012 1  − t"
        },
        {
          "type": "text",
          "bbox": [
            369.90301513671875,
            643.769287109375,
            480.59393310546875,
            667.4495239257812
          ],
          "index": 0,
          "content": "T  , κ \u0013 , (14)"
        }
      ]
    },
    "7": {
      "page_number": 7,
      "elements": [
        {
          "type": "text",
          "bbox": [
            222.78199768066406,
            92.88749694824219,
            480.6198425292969,
            101.85389709472656
          ],
          "index": 0,
          "content": "GenFRC for Non-Exemplar Class-Incremental Learning 7"
        },
        {
          "type": "text",
          "bbox": [
            134.76499938964844,
            118.01194763183594,
            480.65655517578125,
            211.66064453125
          ],
          "index": 0,
          "content": "where  t  is the current epoch,  T  the total number of epochs,  λ 0  = 50 is the initial weight, and  κ  = 0 . 1 is the minimum retention ratio. This scheduling design emphasizes stability during ear"
        },
        {
          "type": "text",
          "bbox": [
            134.76504516601562,
            230.6460418701172,
            286.1567077636719,
            240.608642578125
          ],
          "index": 0,
          "content": "3.4 Integrated Loss Function"
        },
        {
          "type": "text",
          "bbox": [
            134.76504516601562,
            249.6300506591797,
            480.6266784667969,
            271.54766845703125
          ],
          "index": 0,
          "content": "The overall training objective combines classification, feature replay, and feature calibration terms:"
        },
        {
          "type": "text",
          "bbox": [
            230.80105590820312,
            282.1979675292969,
            480.5960693359375,
            293.7836608886719
          ],
          "index": 0,
          "content": "L total  =  L cls  +  L HCVR  +  λ f L FKAM , (15)"
        },
        {
          "type": "text",
          "bbox": [
            134.7650146484375,
            302.9399719238281,
            480.61669921875,
            350.3916320800781
          ],
          "index": 0,
          "content": "where  L cls  denotes the cross-entropy classification loss,  L HCVR  includes both reconstruction and regularization terms derived from the ELBO,  L FKAM  serves as a global regularizer to minimize d"
        },
        {
          "type": "text",
          "bbox": [
            134.7650146484375,
            368.3167724609375,
            229.07955932617188,
            380.27197265625
          ],
          "index": 0,
          "content": "4 Experiments"
        },
        {
          "type": "text",
          "bbox": [
            134.7650146484375,
            392.8430480957031,
            203.98516845703125,
            402.8056335449219
          ],
          "index": 0,
          "content": "4.1 Datasets"
        },
        {
          "type": "text",
          "bbox": [
            134.7650146484375,
            411.82806396484375,
            480.6366882324219,
            517.4315185546875
          ],
          "index": 0,
          "content": "We follow previous works by evaluating our proposed GenFRC model on two widely-used public datasets: CIFAR-100 and TinyImageNet. CIFAR-100 con- sists of 100 classes, with 500 training images and 100 t"
        },
        {
          "type": "text",
          "bbox": [
            134.7650146484375,
            536.4159545898438,
            257.38470458984375,
            546.3785400390625
          ],
          "index": 0,
          "content": "4.2 Evaluation Criteria"
        },
        {
          "type": "text",
          "bbox": [
            134.76498413085938,
            555.4009399414062,
            480.646484375,
            665.0985107421875
          ],
          "index": 0,
          "content": "We evaluate performance using three metrics: Last-stage Accuracy, Average Accuracy, and Average Forgetting. Last-stage Accuracy ( A K ) measures model performance after the final task, indicating its "
        }
      ]
    },
    "8": {
      "page_number": 8,
      "elements": [
        {
          "type": "table",
          "bbox": [
            134.76499938964844,
            150.10699462890625,
            485.208984375,
            330.364013671875
          ],
          "index": 0,
          "label": "表格",
          "confidence": 1.0,
          "refined": true
        },
        {
          "type": "table",
          "bbox": [
            136.66099548339844,
            394.1759948730469,
            299.2539978027344,
            534.3619995117188
          ],
          "index": 1,
          "label": "表格",
          "confidence": 1.0,
          "refined": true
        },
        {
          "type": "table",
          "bbox": [
            136.66099548339844,
            394.1759948730469,
            299.2539978027344,
            534.3619995117188
          ],
          "index": 2,
          "label": "表格",
          "confidence": 1.0,
          "refined": true
        },
        {
          "type": "text",
          "bbox": [
            134.76499938964844,
            92.88749694824219,
            215.2294921875,
            101.85389709472656
          ],
          "index": 0,
          "content": "8 Anonymous"
        },
        {
          "type": "text",
          "bbox": [
            134.76499938964844,
            126.38090515136719,
            480.5768737792969,
            148.29852294921875
          ],
          "index": 0,
          "content": "Table 1: Last-stage accuracy comparison with state-of-the-art methods on CIFAR-100 and TinyImageNet (%)."
        },
        {
          "type": "text",
          "bbox": [
            134.76499938964844,
            358.49493408203125,
            304.2189025878906,
            392.36749267578125
          ],
          "index": 0,
          "content": "Table 2: Average accuracy compari- son with state-of-the-art methods on CIFAR-100 dataset (%)."
        },
        {
          "type": "text",
          "bbox": [
            311.1340026855469,
            358.49493408203125,
            480.59783935546875,
            392.36749267578125
          ],
          "index": 0,
          "content": "Table 3: Average Forgetting compar- ison with state-of-the-art methods on CIFAR-100 dataset (%)."
        },
        {
          "type": "text",
          "bbox": [
            312.1310119628906,
            396.5814514160156,
            491.72747802734375,
            405.5478515625
          ],
          "index": 0,
          "content": "Method 5 stages 10 stages 20 stages"
        },
        {
          "type": "text",
          "bbox": [
            312.1309814453125,
            410.9384765625,
            482.9512634277344,
            531.576904296875
          ],
          "index": 0,
          "content": "LwF [18] 50 . 8 55 . 0 58 . 0 iCaRL-CNN [25] 42 . 8 46 . 5 51 . 0 iCaRL-NME [25] 27 . 8 31 . 9 28 . 8 EEIL [4] 23 . 4 26 . 7 32 . 4 PASS [39] 19 . 6 26 . 6 27 . 8 IL2A [38] 28 . 5 39 . 3 41 . 3 SSRE ["
        },
        {
          "type": "text",
          "bbox": [
            134.76499938964844,
            564.845947265625,
            278.8939514160156,
            574.8085327148438
          ],
          "index": 0,
          "content": "4.3 Implementation Details"
        },
        {
          "type": "text",
          "bbox": [
            134.76498413085938,
            595.3599243164062,
            480.6564636230469,
            665.0985107421875
          ],
          "index": 0,
          "content": "We use the widely adopted ResNet-18 architecture [12] as the backbone network, initialized from scratch. The model is optimized using the Adam optimizer with an initial learning rate of 1  ×  10 − 3  "
        }
      ]
    },
    "9": {
      "page_number": 9,
      "elements": [
        {
          "type": "table",
          "bbox": [
            192.31700134277344,
            350.4200134277344,
            419.96600341796875,
            426.47601318359375
          ],
          "index": 0,
          "label": "表格",
          "confidence": 1.0,
          "refined": true
        },
        {
          "type": "text",
          "bbox": [
            222.78199768066406,
            92.88749694824219,
            480.6198425292969,
            101.85389709472656
          ],
          "index": 0,
          "content": "GenFRC for Non-Exemplar Class-Incremental Learning 9"
        },
        {
          "type": "text",
          "bbox": [
            158.07200622558594,
            248.73789978027344,
            457.2787780761719,
            258.70050048828125
          ],
          "index": 0,
          "content": "Fig. 2: Complete classification accuracy of each stage on CIFAR-100."
        },
        {
          "type": "text",
          "bbox": [
            134.7650146484375,
            285.3249206542969,
            480.5868225097656,
            307.24249267578125
          ],
          "index": 0,
          "content": "seed was set to 2025. All experiments are executed using PyTorch on a single NVIDIA A6000 GPU."
        },
        {
          "type": "text",
          "bbox": [
            201.61700439453125,
            338.6488952636719,
            413.7705993652344,
            348.6114807128906
          ],
          "index": 0,
          "content": "Table 4: Ablation study of different components."
        },
        {
          "type": "text",
          "bbox": [
            134.76499938964844,
            442.846923828125,
            331.0381774902344,
            452.80950927734375
          ],
          "index": 0,
          "content": "4.4 Comparison with state-of-the-arts"
        },
        {
          "type": "text",
          "bbox": [
            134.76499938964844,
            463.67291259765625,
            480.6662902832031,
            665.0984497070312
          ],
          "index": 0,
          "content": "Main Results  We compare GenFRC method with various NECIL approaches, including LwF [18], DssSIL [3], ABD [29], PASS [39], IL2A [38], SSRE [40], R- DFCIL [8], EDG [10], FeTrIL [22], FCS [16], SEED [26"
        },
        {
          "type": "image",
          "bbox": [
            152.0570068359375,
            115.83504486083984,
            463.3146057128906,
            238.1929931640625
          ],
          "index": 0
        }
      ]
    },
    "10": {
      "page_number": 10,
      "elements": [
        {
          "type": "text",
          "bbox": [
            134.76499938964844,
            92.88749694824219,
            215.2294921875,
            101.85389709472656
          ],
          "index": 0,
          "content": "10 Anonymous"
        },
        {
          "type": "text",
          "bbox": [
            134.76499938964844,
            214.6988983154297,
            480.5867614746094,
            236.61651611328125
          ],
          "index": 0,
          "content": "Fig. 3: Top-1 accuracy of the old classes, new classes, and the performance gain caused by the integration of HCVR from different stages."
        },
        {
          "type": "text",
          "bbox": [
            134.76499938964844,
            349.11993408203125,
            480.6268005371094,
            371.0375061035156
          ],
          "index": 0,
          "content": "Fig. 4: Average Euclidean distance between the old network and the new network at each stage on CIFAR-100."
        },
        {
          "type": "text",
          "bbox": [
            134.76499938964844,
            396.0489196777344,
            480.6565246582031,
            504.1714172363281
          ],
          "index": 0,
          "content": "Tables 2 and 3 report Average Accuracy (  ¯ A ) and Average Forgetting (AF) for CIFAR-100. GenFRC attains the highest  ¯ A  across all settings, with values of 72.6%, 70.8%, and 69.2% for the 5, 10, a"
        },
        {
          "type": "text",
          "bbox": [
            134.76499938964844,
            527.2618408203125,
            480.62664794921875,
            597.0004272460938
          ],
          "index": 0,
          "content": "Average Curve  Fig. 2 illustrates the stage-wise top-1 accuracy on CIFAR-100 for the 5-, 10-, and 20-stage settings. Throughout the entire trajectory, GenFRC consistently outperforms the other methods"
        },
        {
          "type": "text",
          "bbox": [
            134.76499938964844,
            620.0908813476562,
            244.67242431640625,
            630.053466796875
          ],
          "index": 0,
          "content": "4.5 Ablation Studies"
        },
        {
          "type": "text",
          "bbox": [
            134.76499938964844,
            643.1808471679688,
            480.6265563964844,
            665.0984497070312
          ],
          "index": 0,
          "content": "Table 4 reports the quantitative ablation results on CIFAR-100, while Figs. 3 and4 provide a stage-wise breakdown of the effects of each component."
        },
        {
          "type": "image",
          "bbox": [
            160.7010040283203,
            115.83491516113281,
            454.65850830078125,
            204.15399169921875
          ],
          "index": 0
        },
        {
          "type": "image",
          "bbox": [
            160.7010040283203,
            251.24908447265625,
            454.648193359375,
            338.57501220703125
          ],
          "index": 1
        }
      ]
    },
    "11": {
      "page_number": 11,
      "elements": [
        {
          "type": "table",
          "bbox": [
            134.76499938964844,
            412.7760009765625,
            474.083984375,
            482.4070129394531
          ],
          "index": 0,
          "label": "表格",
          "confidence": 1.0,
          "refined": true
        },
        {
          "type": "table",
          "bbox": [
            134.76499938964844,
            428.0069885253906,
            474.083984375,
            482.4070129394531
          ],
          "index": 1,
          "label": "表格",
          "confidence": 1.0,
          "refined": true
        },
        {
          "type": "text",
          "bbox": [
            222.78199768066406,
            92.88749694824219,
            480.6198425292969,
            101.85389709472656
          ],
          "index": 0,
          "content": "GenFRC for Non-Exemplar Class-Incremental Learning 11"
        },
        {
          "type": "text",
          "bbox": [
            134.76499938964844,
            118.01194763183594,
            480.6366271972656,
            247.52667236328125
          ],
          "index": 0,
          "content": "Effectiveness of HCVR:  We present a visualization of classification accuracy on both old and new classes in Fig. 3, along with the corresponding performance gain across incremental stages. Notably, t"
        },
        {
          "type": "text",
          "bbox": [
            134.76499938964844,
            264.76007080078125,
            480.6466064453125,
            382.3195495605469
          ],
          "index": 0,
          "content": "Effectiveness of FKAM:  We visualize the average Euclidean distance between the feature centers extracted by the old network and the new network across dif- ferent incremental stages on CIFAR-100, wit"
        },
        {
          "type": "text",
          "bbox": [
            342.26300048828125,
            389.0499267578125,
            480.0925598144531,
            410.9674987792969
          ],
          "index": 0,
          "content": "Table 6: Impact of  λ f  and  D rff evaluated on CIFAR-100."
        },
        {
          "type": "text",
          "bbox": [
            134.76499938964844,
            506.39593505859375,
            333.9870910644531,
            516.3585205078125
          ],
          "index": 0,
          "content": "4.6 Effect of Hyperparameter Settings"
        },
        {
          "type": "text",
          "bbox": [
            134.76498413085938,
            523.6289672851562,
            480.6564636230469,
            665.0985107421875
          ],
          "index": 0,
          "content": "To investigate the influence of FKAM and its associated hyperparameter con- figurations, we analyse the kernel variant, the MMD weight  λ f , and the RFF dimension  D rff . The corresponding quantitat"
        }
      ]
    },
    "12": {
      "page_number": 12,
      "elements": [
        {
          "type": "text",
          "bbox": [
            134.76499938964844,
            92.88749694824219,
            215.2294921875,
            101.85389709472656
          ],
          "index": 0,
          "content": "12 Anonymous"
        },
        {
          "type": "text",
          "bbox": [
            134.7650146484375,
            118.01194763183594,
            480.6466979980469,
            211.66064453125
          ],
          "index": 0,
          "content": "MMD Regularization Weight  λ f :  Table 6 shows that increasing  λ f from 50 to 150 consistently degrades performance. A large weight likely over- constrains the feature alignment objective, hindering"
        },
        {
          "type": "text",
          "bbox": [
            134.76504516601562,
            230.6597442626953,
            219.31219482421875,
            242.6149444580078
          ],
          "index": 0,
          "content": "5 Conclusion"
        },
        {
          "type": "text",
          "bbox": [
            134.76504516601562,
            254.76499938964844,
            480.66644287109375,
            396.2344970703125
          ],
          "index": 0,
          "content": "In this paper, we propose a Generative Feature Replay and Calibration (Gen- FRC) method to address the challenging problem of catastrophic forgetting in non-exemplar class-incremental learning (NECIL)"
        },
        {
          "type": "text",
          "bbox": [
            134.76504516601562,
            415.2326354980469,
            197.6971893310547,
            427.1878356933594
          ],
          "index": 0,
          "content": "References"
        },
        {
          "type": "text",
          "bbox": [
            139.37203979492188,
            438.78045654296875,
            480.7319641113281,
            664.8807983398438
          ],
          "index": 0,
          "content": "1. Arjovsky, M., Chintala, S., Bottou, L.: Wasserstein generative adversarial networks. In: International conference on machine learning. pp. 214–223. PMLR (2017) 2. Azizi, S., Mustafa, B., Ryan, F., "
        }
      ]
    },
    "13": {
      "page_number": 13,
      "elements": [
        {
          "type": "text",
          "bbox": [
            222.78199768066406,
            92.88749694824219,
            480.6198425292969,
            101.85389709472656
          ],
          "index": 0,
          "content": "GenFRC for Non-Exemplar Class-Incremental Learning 13"
        },
        {
          "type": "text",
          "bbox": [
            134.76499938964844,
            118.79051208496094,
            480.7425842285156,
            664.8807373046875
          ],
          "index": 0,
          "content": "8. Gao, Q., Zhao, C., Ghanem, B., Zhang, J.: R-dfcil: Relation-guided representa- tion learning for data-free class incremental learning. In: European Conference on Computer Vision. pp. 423–439. Sprin"
        }
      ]
    },
    "14": {
      "page_number": 14,
      "elements": [
        {
          "type": "text",
          "bbox": [
            134.76499938964844,
            92.88749694824219,
            215.2294921875,
            101.85389709472656
          ],
          "index": 0,
          "content": "14 Anonymous"
        },
        {
          "type": "text",
          "bbox": [
            134.76499938964844,
            118.79051208496094,
            480.7247009277344,
            659.6417236328125
          ],
          "index": 0,
          "content": "24. Rahimi, A., Recht, B.: Random features for large-scale kernel machines. Advances in neural information processing systems  20  (2007) 25. Rebuffi, S.A., Kolesnikov, A., Sperl, G., Lampert, C.H.: i"
        }
      ]
    }
  }
}