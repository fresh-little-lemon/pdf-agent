```html
<html><body>
<p data-bbox="84 102 537 140">instances into a latent feature space in which semantic similarity is preserved.</p> 
 <p data-bbox="84 140 539 276">For each class label $y \in \mathcal{Y}$, we extract the feature representations of all instances labeled $y$, denoted as $\left\{\mathbf{z}_{i} \mid i \in C(y)\right\}$, where $C(y)$ is the index set of all samples belonging to class $y$. Within each class, we apply $k$-means clustering to partition the feature space into semantically coherent groups, indexed by cluster $r$. Let $C(y, r) \subseteq C(y)$ denote the index set of samples assigned to the $r$-th cluster of class $y$.</p> 
 <p data-bbox="84 276 539 320">We define the cluster centroid as the mean vector of all feature embeddings in that cluster:</p> 
 <div class="formula" data-bbox="175 320 445 364"><img data-bbox="175 320 445 364"/><div>$$\mathbf{z}_{\text {centroid }}(y, r)=\frac{1}{|C(y, r)|} \sum_{i \in C(y, r)} \mathbf{z}_{i} .$$</div></div> 
 <p data-bbox="73 379 539 406">To enhance computational efficiency, we compute the squared Euclidean distance between a given sample $\mathbf{z}_{i}$ and any cluster centroid $\mathbf{z}_{\text {centroid }}\left(y^{\prime}, r^{\prime}\right)$ using an optimized inner-product formulation:</p> 
 <div class="formula" data-bbox="110 406 539 478"><img data-bbox="110 406 539 478"/><div>$$\begin{aligned} &amp; \left\|\mathbf{z}_{i}-\mathbf{z}_{\text {centroid }}\left(y^{\prime}, r^{\prime}\right)\right\|^{2} \\ &amp; \quad=\left\|\mathbf{z}_{i}\right\|^{2}+\left\|\mathbf{z}_{\text {centroid }}\left(y^{\prime}, r^{\prime}\right)\right\|^{2}-2 \cdot \mathbf{z}_{i}^{\top} \mathbf{z}_{\text {centroid }}\left(y^{\prime}, r^{\prime}\right) . \end{aligned}$$</div></div> 
 <p data-bbox="73 478 539 579">This approach avoids redundant calculations by allowing precomputation of norm terms and leveraging efficient matrix multiplication on GPUs, thus reducing overall complexity from $O(B K D)$ to a cost dominated by batched matrix operations, where $B$ is the batch size, $K$ the number of clusters, and $D$ the embedding dimensionality.</p> 
 <p data-bbox="73 579 539 647">Within each cluster $(y, r)$, we identify a representative sample as the data point closest to the cluster centroid:</p> 
 <div class="formula" data-bbox="175 647 472 691"><img data-bbox="175 647 472 691"/><div>$$\mathbf{x}_{\mathrm{rep}}(y, r)=\underset{i \in C(y, r)}{\arg \min }\left\|\mathbf{z}_{i}-\mathbf{z}_{\text {centroid }}(y, r)\right\| .$$</div></div> 
 <p data-bbox="73 691 539 776">For a given sample $\mathbf{z}_{i}$ with class label $y_{i}$, we search across all clusters belonging to different classes $y^{\prime} \neq y_{i}$. Among these, we select the cluster whose centroid is closest to $\mathbf{z}_{i}$ and adopt the corresponding representative sample as its hard-init counterpart:</p> 
 <div class="formula" data-bbox="102 785 539 839"><img data-bbox="102 785 539 839"/><div>$$\mathbf{x}_{\text {hard }}^{\text {init }}\left(\mathbf{z}_{i}\right)=\mathbf{x}_{\mathrm{rep}}\left(\underset{\left.\left(y^{\prime}, r^{\prime}\right): y^{\prime} \neq y_{i}\right)}{\arg \min }\left\|\mathbf{z}_{i}-\mathbf{z}_{\text {centroid }}\left(y^{\prime}, r^{\prime}\right)\right\|\right) .$$</div></div> 
 <p data-bbox="73 839 539 958">This asymmetric reconstruction strategy identifies, for each input sample, the most semantically similar centroid from a different class and retrieves the representative example from that cluster. By focusing on samples that lie near class boundaries in the embedding space, the procedure introduces semantically ambiguous examples into the training process.</p> 
 <h2 data-bbox="84 980 525 1019"> D. Mutual Sample Inversion (MSI) for Adaptive Generation of Boundary-Refining Samples</h2> 
 <p data-bbox="73 1026 539 1185">Having identified hard-init samples via cross-centroid matching, DiffCL then applies a label-guided reverse diffusion process to craft more challenging training examples. The key idea is to transfer a sample’s low-level features from its original (such as “negative”) class to a target (such as “positive”) class while retaining certain granular details of the source. As a result, the newly synthesized samples lie closer to the decision boundary, thereby challenging the model to refine its classification rule in high-ambiguity regions.</p> 
 <p data-bbox="73 1185 539 1277">Concretely, let $\mathbf{x}_{\text {hard }}^{\text {init }}$ be a selected hard-init sample, whose class label we denote as $\mathbf{y}^{-}$. We aim to invert $\mathbf{x}_{\text {hard }}^{\text {init }}$ toward a new class label $\mathbf{y}^{+} \neq \mathbf{y}^{-}$. This process starts by progressively adding Gaussian noise in the forward diffusion:</p> 
 <div class="formula" data-bbox="102 1277 539 1321"><img data-bbox="102 1277 539 1321"/><div>$$q\left(\mathbf{x}_{\text {hard }, t}^{\text {init }} \mid \mathbf{x}_{\text {hard }, t-1}^{\text {init }}\right)=\mathcal{N}\left(\sqrt{\alpha_{t}} \mathbf{x}_{\text {hard }, t-1}^{\text {init }},\left(1-\alpha_{t}\right) \mathbf{I}\right),$$</div></div> 
 <p data-bbox="73 1338 539 1382">where $\sqrt{\alpha_{t}} \mathbf{x}_{\text {hard }, t-1}^{\text {init }}$ represents the partially corrupted sample at time $t$, and $\left(1-\alpha_{t}\right) \mathbf{I}$ is the variance of the added noise. After</p> 
 <div class="image" data-bbox="553 93 951 282"><img data-bbox="553 93 951 282"/></div> 
 <p data-bbox="553 287 1006 330">Fig. 3: Mutual Sample Inversion (MSI) for Adaptive Generation of Boundary-Refining Samples.</p> 
 <p data-bbox="553 368 1006 406">sufficiently many steps $t$, the sample converges to an isotropic Gaussian distribution.</p> 
 <p data-bbox="553 406 1006 443">The label-guided reverse diffusion then incrementally denoises this corrupted sample under the supervision of the target label $\mathbf{y}^{+}$.</p> 
 <p data-bbox="618 443 620 469">,</p> 
 <div class="formula" data-bbox="620 443 936 538"><img data-bbox="620 443 936 538"/><div>$$\begin{aligned} p_{\theta}\left(\mathbf{x}_{\text {hard }, t-1}^{\text {init }} \mid \mathbf{x}_{\text {hard }, t}^{\text {init }}, \mathbf{y}^{+}\right) &amp; \\ = &amp; \mathcal{N}\left(\mu_{\theta}\left(\mathbf{x}_{\text {hard }, t}^{\text {init }}, \mathbf{y}^{+}, t\right), \Sigma_{\theta}\left(\mathbf{x}_{\text {hard }, t}^{\text {init }}, \mathbf{y}^{+}, t\right)\right) . \end{aligned}$$</div></div> 
 <p data-bbox="553 545 990 583">Conversely, if the source sample is from a positive class $\mathbf{y}^{+}$and we target a negative class $\mathbf{y}^{-}$, the reverse diffusion is formulated as:</p> 
 <div class="formula" data-bbox="620 583 936 656"><img data-bbox="620 583 936 656"/><div>$$\begin{aligned} p_{\theta}\left(\mathbf{x}_{\text {hard }, t-1}^{\text {init }} \mid \mathbf{x}_{\text {hard }, t}^{\text {init }}, \mathbf{y}^{-}\right) &amp; \\ = &amp; \mathcal{N}\left(\mu_{\theta}\left(\mathbf{x}_{\text {hard }, t}^{\text {init }}, \mathbf{y}^{-}, t\right), \Sigma_{\theta}\left(\mathbf{x}_{\text {hard }, t}^{\text {init }}, \mathbf{y}^{-}, t\right)\right) . \end{aligned}$$</div></div> 
 <p data-bbox="553 656 1006 757">Here, $\mu_{\theta}(\cdot)$ and $\Sigma_{\theta}(\cdot)$ are the mean and covariance predicted by a pretrained conditional diffusion model. Through repeated denoising steps, the low-frequency structures from the source class are partly preserved, while higher-level semantics adapt to the target label $\mathbf{y}$.</p> 
 <p data-bbox="553 757 1006 927">This bidirectional mechanism simultaneously supports hard positive and hard negative sample generation. For hard positives, a data point from the negative class $\mathbf{y}^{-}$is first diffused forward and then reconstructed under the label $\mathbf{y}^{+}$, yielding $\mathbf{x}_{\text {hard }}^{+}$, which integrates low-level structural attributes of $\mathbf{y}^{-}$with the semantic identity of $\mathbf{y}^{+}$. Positioned near the decision boundary, this sample demands fine-grained discrimination. By analogy, hard negatives are generated by diffusing a sample from $\mathbf{y}^{+}$and inverting it with respect to $\mathbf{y}^{-}$, resulting in $\mathbf{x}_{\text {hard }}^{-}$, which combines traits from both classes.</p> 
 <h2 data-bbox="553 959 990 1019"> E. Adaptive Weighted Contrastive Loss (AWCL) for Information-Sensitive Representation Optimization</h2> 
 <p data-bbox="553 1021 1006 1172">A key limitation of CL lies in its uniform weighting of all positive pairs, disregarding the variability in their difficulty. In realworld defect classification, where class imbalance and inter-class overlap are prevalent, this uniform treatment leads the model to focus on easy positive pairs while overlooking harder ones near class boundaries—crucial for learning robust and discriminative features.</p> 
 <p data-bbox="553 1172 1006 1368">To address this, we adaptively reweight positive pairs based on difficulty, assigning greater importance to those that yield more informative gradients. This strategy sharpens decision boundaries in ambiguous regions, aligning with the principle of focusing on harder cases to enhance learning. We realize this via a learnable parameter $\epsilon&gt;0$, initialized (e.g., to 0.1 ) and updated during training. A weight $\omega=\exp (-\epsilon)$ is assigned to each positive pair, such that gradient descent dynamically downweights trivial pairs and emphasizes those requiring finer distinction, thereby guiding the model toward more discriminative representations.</p> 
 <p data-bbox="553 1368 1006 1412">Let $\mathbf{z}_{i}$ be the embedding of an anchor sample $i$, and $\mathbf{z}_{p}$ be that of a corresponding positive sample $p$. Denote by $P(i)$ the set of all</p> 
</body></html>
```