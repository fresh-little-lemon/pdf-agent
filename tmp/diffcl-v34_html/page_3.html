```html
<html><body>
<div class="image" data-bbox="148 92 937 436"><img data-bbox="148 92 937 435"/></div> 
 <p data-bbox="80 440 1005 522">Fig. 1: Architecture of the Diffusion Contrastive Learning (DiffCL) Framework. The proposed DiffCL integrates conditional diffusion modeling with supervised contrastive learning through three key components: (1) Cross-Centroid Asymmetric Reconstruction, (2) Mutual Sample Inversion, and (3) Adaptive Weighted Contrastive Loss. DiffCL enhances feature discriminability by generating boundary-aware synthetic samples while maintaining semantic consistency through label-conditioned diffusion processes.</p> 
 <p data-bbox="80 560 536 642">this shows the overlap appears in the classification. Such overlap introduces significant ambiguity, impeding reliable label assignment by the classifier, hence the classification model $f(\mathbf{x})$ becomes uncertain and prone to make misclassifications.</p> 
 <h2 data-bbox="82 660 204 680">B. Prerequisites</h2> 
 <p data-bbox="80 684 536 764">To mitigate the ambiguity arising from overlapping of class decision boundaries, we propose the Diffusion Contrastive Learning (DiffCL) framework. DiffCL incorporates two foundational hypotheses that guide both its structural design and operational principles:</p> 
 <p data-bbox="80 772 536 855">Hypothesis 1 (Semantic Distance Threshold). Let $\psi(\mathbf{x}, \mathbf{x}_{\text {aug }})$ be a semantic distance metric between an original image $\mathbf{x}$ and its augmented version $\mathbf{x}_{\mathrm{aug}}$. The effectiveness of contrastive learning improves when this distance exceeds a threshold $\delta$ :</p> 
 <div class="formula" data-bbox="252 860 364 889"><img data-bbox="252 860 364 889"/><div>$$\psi\left(\mathbf{x}, \mathbf{x}_{\mathrm{aug}}\right)&gt;\delta,$$</div></div> 
 <p data-bbox="80 893 536 934">where $\delta&gt;0$ is the minimal semantic difference required to produce meaningful augmented samples.</p> 
 <p data-bbox="80 938 536 1024">Hypothesis 2 (Image Decomposition). Any defect image $\mathbf{x}$ can be decomposed into three parts: a blurred (low-frequency) component $\mathbf{x}_{\mathrm{bp}}$, a detailed (high-frequency) component $\mathbf{x}_{\mathrm{dp}}$, and a categorical component $\mathbf{x}_{\mathrm{cp}}$ encoding class-specific semantics:</p> 
 <div class="formula" data-bbox="232 1029 386 1057"><img data-bbox="232 1029 386 1057"/><div>$$\mathbf{x}=\mathbf{x}_{\mathrm{bp}} \oplus \mathbf{x}_{\mathrm{dp}} \oplus \mathbf{x}_{\mathrm{cp}},$$</div></div> 
 <p data-bbox="80 1062 536 1103">where $\oplus$ denotes a suitable composition operator (e.g., element-wise addition).</p> 
 <p data-bbox="80 1110 1005 1184">Traditional data augmentation methods often fail to fulfill Hypothesis 1 because they introduce uncontrolled semantic drift [9]. Inspired by DDPM [21], DiffCL employs a class-conditional diffusion process to extract the blurred component $\mathbf{x}_{\mathrm{bp}}$ while preserving essential lowfrequency information. Specifically, the forward diffusion progressively adds noise to an original image $\mathbf{x}_{0}$ over $T$ steps, eventually approximating an isotropic Gaussian distribution yet retaining the low-frequency structure. After $t$ steps of noising, we sample $\mathbf{x}_{\mathrm{bp}}$ as:</p> 
 <div class="formula" data-bbox="162 1189 454 1227"><img data-bbox="162 1189 454 1227"/><div>$$q\left(\mathbf{x}_{\mathrm{bp}} \mid \mathbf{x}_{0}\right)=\mathcal{N}\left(\mathbf{x}_{\mathrm{bp}} ; \sqrt{\bar{\alpha}_{t}} \mathbf{x}_{0},\left(1-\bar{\alpha}_{t}\right) \mathbf{I}\right),$$</div></div> 
 <p data-bbox="80 1227 536 1268">where $\bar{\alpha}_{t}=\prod_{i=1}^{t} \alpha_{i}$, each $\alpha_{i} \in(0,1)$ governs the amount of noise introduced at step $i$, and $I$ denoting the identity matrix.</p> 
 <div class="image" data-bbox="569 549 998 784"><img data-bbox="569 549 998 784"/></div> 
 <p data-bbox="552 784 998 828">Fig. 2: Cross-Class Asymmetric Reconstruction (CCAR) for Semantically-Aware Hard Sample Initialization.</p> 
 <p data-bbox="552 864 1005 947">To generate semantically distinct samples while retaining label consistency, we merge the blurred component from a negative sample with the detailed component from a positive sample. For the positive augmentation:</p> 
 <div class="formula" data-bbox="689 940 866 972"><img data-bbox="689 940 866 972"/><div>$$\mathbf{x}_{\mathrm{aug}}^{+}=\mathbf{x}_{\mathrm{bp}}^{-} \oplus \mathbf{x}_{\mathrm{dp}}^{+} \oplus \mathbf{x}_{\mathrm{cp}}^{+},$$</div></div> 
 <p data-bbox="552 974 783 998">and for the negative augmentation:</p> 
 <div class="formula" data-bbox="689 998 899 1032"><img data-bbox="689 998 899 1032"/><div>$$\mathbf{x}_{\mathrm{aug}}^{-}=\mathbf{x}_{\mathrm{bp}}^{+} \oplus \mathbf{x}_{\mathrm{dp}}^{-} \oplus \mathbf{x}_{\mathrm{cp}}^{-},$$</div></div> 
 <p data-bbox="552 1036 1005 1182">where $\mathbf{x}_{\mathrm{bp}}^{\pm}$is derived from Eq. (5), and $\mathbf{x}_{\mathrm{dp}}^{\mp}$is obtained under the semantic guidance of $\mathbf{x}_{\mathrm{cp}}^{\pm}$ (see Eq. (13) below). Through this design, Hypothesis 1 and 2 are jointly addressed: the newly generated samples remain label-consistent while achieving substantial variation from the originals. This process endows DiffCL with stronger boundary awareness, aiding in disentangling heavily overlapping class distributions.</p> 
 <h2 data-bbox="552 1206 951 1248">C. Cross-Class Asymmetric Reconstruction (CCAR) for Semantically-Aware Hard Sample Initialization</h2> 
 <p data-bbox="552 1252 1005 1352">The process of generating hard samples in DiffCL begins with the identification of a challenging subset, termed hard-init samples. This subset is constructed via an Euclidean-distance-based selection mechanism aimed at capturing semantically ambiguous relationships. To this end, we first leverage the backbone network to map input</p> 
</body></html>
```