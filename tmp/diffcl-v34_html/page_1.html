```html
<html><body>
<h1 data-bbox="116 105 972 170">DiffCL: Diffusion Contrastive Learning for Strip Steel Surface Defect Classification</h1> 
 <p data-bbox="84 239 538 729"></p> 
 <address class="author" data-bbox="84 239 538 729"><p data-bbox="84 239 538 729">Abstract-Accurate and automated classification of surface defects in steel is of paramount importance for modern steel manufacturing. However, this task remains challenging due to the high visual homogeneity, which induces ambiguous feature representations across defect categories. Contrastive learning (CL) has shown promise by constructing positive and negative pairs to mitigate cross-class feature similarity and learn discriminative representations. Nonetheless, CL faces an inherent limitation in balancing semantic consistency and sample diversity, resulting in suboptimal decision boundaries and reduced model generalization. To address this issue, we propose the Diffusion Contrastive Learning (DiffCL) framework, which progressively refines decision boundaries by constructing a rich manifold of hard samples. DiffCL consists of three key components: Cross-Class Asymmetric Reconstruction (CCAR) identifies an initial pool of hard samples by selectively reconstructing input instances with inter-class feature perturbations, enriching the input space with ambiguous yet semantically meaningful variations; Mutual Sample Inversion (MSI) dynamically transforms these samples during training to generate boundary-adjacent variations that reflect the evolving structure of the feature space, and Adaptive Weighted Contrastive Loss (AWCL) leverages information gradient cues to assign sample-specific weights during optimization, enabling the model to focus more on samples that are harder to learn and more informative for representation refinement. Extensive experiments on four benchmark datasets across twelve quantitative metrics demonstrate that DiffCL consistently outperforms state-of-the-art methods, highlighting its superior ability to achieve precise, stable, and robust steel surface defect classification. Code will be released upon acceptance at: https://github.com/Ethan2186/DiffCL.</p></address> 
 <p data-bbox="84 738 538 781">Index Terms-Industrial defect classification, Strip steel, Contrastive learning, Diffusion models</p> 
 <h2 data-bbox="237 820 383 840"> I. INTRODUCTION</h2> 
 <p data-bbox="84 853 538 1152">Accurate and automated classification of surface defects in steel is of paramount importance for modern steel manufacturing [49]. With the rapid development of industries such as automotive and defense, increasingly stringent quality standards are being imposed on strip steel products [14]. However, the quality of steel strips is often compromised by impurities in raw materials and deviations in manufacturing processes, leading to the occurrence of defects such as holes and pits. These defects not only degrade the structural integrity and corrosion resistance of the steel but also undermine the safety and reliability of downstream applications [34]. In practical industrial settings, precise identification of defect categories enables manufacturers to trace the underlying causes of defects, thereby facilitating targeted optimization of raw material selection and process control. Such defect-informed process improvements can ultimately enhance the overall quality of strip steel production.</p> 
 <p data-bbox="84 1152 538 1352">Accurate and automated classification of strip steel surface defects remains challenging due to the high degree of visual homogeneity among distinct defect categories. This homogeneity is frequently exacerbated by variations in illumination conditions, viewing angles, and background textures [22], which obscure class-discriminative features and contribute to the formation of overlapping feature distributions. The considerable overlap in visual characteristics leads to the degradation of decision boundaries, rendering them increasingly ambiguous and thereby reducing inter-class separability [40]. This degradation weakens the discriminative capability of conventional</p> 
 <p data-bbox="553 239 1005 280">classification models, impeding their ability to identify subtle variations in steel surface defects.</p> 
 <p data-bbox="553 280 1007 718">Contrastive learning (CL) mitigates cross-class feature similarity by constructing positive and negative pairs to learn discriminative representations. Early works [4], [41] focused on minimizing distances between positive pairs while maximizing those between negatives but suffered from inefficient sampling and slow convergence. Current CL approaches [6], [15], enhance instance discrimination by maximizing agreement between augmented views of the same image but require extensive computational resources or suffer from representation collapse and sensitivity to momentum tuning, which limit their practical deployment. SupCon [26] enhances self-supervised paradigm with label supervision, promoting intra-class compactness. However, it is susceptible to class imbalance and label noise, which can impair performance in real-world scenarios. Recent advancements [12], [23] have been proposed to mitigate these limitations by introducing adversarial or stylistic variations to enrich the training distribution with challenging samples. However, they face an inherent trade-off: geometric transformations preserve semantic fidelity but offer limited diversity, whereas style-based augmentations increase diversity at the expense of semantic consistency. Achieving both simultaneously remains challenging, particularly in industrial defect classification, where subtle texture variations define fine-grained inter-class boundaries.</p> 
 <p data-bbox="553 718 1007 1139">To mitigate cross-class ambiguity, we propose to construct a rich manifold of hard samples near decision boundaries to facilitate the progressive refinement of precise and robust class separations. These boundary-adjacent samples challenge the model with minimal interclass variation, forcing it to learn subtle discriminative cues that are often overlooked, thereby enhancing both classification sharpness and resilience to ambiguity. Building on this insight, we propose Diffusion Contrastive Learning (DiffCL) framework, which consists of three components: Cross-Class Asymmetric Reconstruction (CCAR), Mutual Sample Inversion (MSI), and Adaptive Weighted Contrastive Loss (AWCL). CCAR identifies an initial pool of hard samples by selectively reconstructing input instances with inter-class feature perturbations, enriching the input space with ambiguous yet semantically meaningful variations. MSI dynamically transforms these samples during training to generate boundary-adjacent variations that reflect the evolving structure of the feature space. Finally, AWCL leverages information gradient cues to assign sample-specific weights during optimization, enabling the model to focus more on samples that are harder to learn and more informative for representation refinement. DiffCL achieves state-of-the-art performance across four datasets. Our main contributions are summarized as follows:</p> 
 <ul data-bbox="568 1144 1005 1324"><li data-bbox="568 1144 1005 1205">We propose to address cross-class ambiguity by constructing a dense spectrum of hard samples near decision boundaries to delineate class margins.</li><li data-bbox="568 1205 1005 1244">We propose Cross-Class Asymmetric Reconstruction to identify semantic hard samples.</li><li data-bbox="568 1244 1005 1284">We propose Mutual Sample Inversion to construct boundaryrefining samples.</li><li data-bbox="568 1284 1005 1324">We propose Adaptive Weighted Contrastive Loss to enhance feature refinement.</li></ul> 
 <p data-bbox="568 1329 1005 1352">The rest of this article is organized as follows. Section II sum-</p> 
</body></html>
```